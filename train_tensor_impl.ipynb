{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Found embeddings: ', 31105, '/', 42514)\n",
      "('word_index len = ', 41522)\n",
      "('time-taken for FP: ', 1.8994669914245605)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile,join\n",
    "import xml.etree.cElementTree as ET\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import math\n",
    "from PyRouge.pyrouge import Rouge\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import FileProcess as fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "embedding_size = 50\n",
    "learning_rate = 0.1\n",
    "context_window = 5\n",
    "cnn_filter_size = 100\n",
    "threshold = 0.5\n",
    "batch_size = 10\n",
    "vocabulary_size=400000\n",
    "#embedding_filename = \"glove_6B_200d.txt\"\n",
    "embedding_filename = \"word2vec.model\"   ##Trying with pre-trained word2vec model and training it on legal dataset\n",
    "training_epochs=1\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"corpus/fulltext/\"\n",
    "files = [f for f in os.listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "files_2006=[];\n",
    "files_2007=[];\n",
    "files_2008=[];\n",
    "files_2009=[];\n",
    "\n",
    "for file in files:\n",
    "    if file.split('_')[0]=='06':\n",
    "        files_2006.append(file);\n",
    "    if file.split('_')[0]=='07':\n",
    "        files_2007.append(file);\n",
    "    if file.split('_')[0]=='08':\n",
    "        files_2008.append(file);\n",
    "    if file.split('_')[0]=='09':\n",
    "        files_2009.append(file);\n",
    "\n",
    "#print(fp.get_sentences(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list=set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "##########loading dev2vec=============\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('doc2vec_models_50len/doc2vec.model')\n",
    "docvec = d2v_model.docvecs[files[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    #print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    dvocab, w2v,word_index  = read_data(filename);\n",
    "    return w2v,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array = fp.embedding_array\n",
    "word_index = fp.word_index\n",
    "\n",
    "reverseWordIndex = {}\n",
    "for key in word_index:\n",
    "    reverseWordIndex[word_index[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO avoid downloading tf hub modules everytime\n",
    "##  export TFHUB_CACHE_DIR=/usr/local/bin\n",
    "\n",
    "#sen2vec embeddings\n",
    "def sen2vec1(sentences):\n",
    "    embed = hub.Module(module_url)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        sentenceEmbeddings = session.run(embed(sentences))    \n",
    "        return sentenceEmbeddings\n",
    "\n",
    "    \n",
    "def sent2vector(sent):\n",
    "    words = word_tokenize(sent.lower())\n",
    "    emb=[]\n",
    "    for w in words:\n",
    "        if w in embedding_array.keys():\n",
    "            emb.append(embedding_array.get(w));\n",
    "        else:\n",
    "            emb.append(embedding_array.get('unk'));\n",
    "    return np.mean(np.array(emb),axis=0)\n",
    "\n",
    "def get_sent_embeddings(sent_list):\n",
    "    out_vec=np.zeros((len(sent_list)-1,200))\n",
    "    for i  in range(len(sent_list)-1):\n",
    "        out_vec[i]=sent2vector(sent_list[i]);\n",
    "    return out_vec\n",
    "    \n",
    "# sentence_embeddings = sen2Vec(['UWA was ordered to pay the costs of Dr Gra','Sirtex succeeded in its cross-claim against Dr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(file):\n",
    "    #with open(dirpath + file, 'r',encoding=\"utf-8\", errors='replace') as f:\n",
    "    with open(dirpath + file, 'r') as f:\n",
    "        data=str(f.read());\n",
    "        data=data.lower()\n",
    "        data = data.replace(\"\\\"id=\", \"id=\\\"\");\n",
    "        data=data.replace(\"\\n\",\"\")\n",
    "        data=data.replace('\".*?=.*?\"', \"\",)\n",
    "        data=data.replace(\"&\",\"\");\n",
    "        xml = ET.fromstring(str(data))\n",
    "        name=None;\n",
    "        rows_list=[];\n",
    "        catchphrases=[];\n",
    "        sentences=[];\n",
    "        for child in xml:\n",
    "            if child.tag==\"catchphrases\":\n",
    "                for catchphrase in child:\n",
    "                    id=catchphrase.attrib.get(\"id\")\n",
    "                    #print(catchphrase.text)\n",
    "                    catchphrases.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":catchphrase.text})\n",
    "                    #catchphrases+=tokenizer.tokenize(catchphrase.text)\n",
    "            if child.tag==\"sentences\":\n",
    "                for sentence in child:\n",
    "                    id = sentence.attrib.get(\"id\")\n",
    "                    sentences.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":sentence.text})\n",
    "                    #sentences+=tokenizer.tokenize(sentence.text)\n",
    "    return sentences,catchphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(        Id     file_id  is_catchword                                words\n",
      "0       s0  07_209.xml             0      [9205, 3424, 27288, 9330, 9272]\n",
      "1       s0  07_209.xml             1     [3424, 27288, 9330, 9272, 13945]\n",
      "2       s0  07_209.xml             0    [27288, 9330, 9272, 13945, 24684]\n",
      "3       s0  07_209.xml             0     [9330, 9272, 13945, 24684, 1109]\n",
      "4       s0  07_209.xml             0    [9272, 13945, 24684, 1109, 27288]\n",
      "5       s0  07_209.xml             0   [13945, 24684, 1109, 27288, 34485]\n",
      "6       s0  07_209.xml             0   [24684, 1109, 27288, 34485, 39252]\n",
      "7       s0  07_209.xml             0    [1109, 27288, 34485, 39252, 1109]\n",
      "8       s0  07_209.xml             0   [27288, 34485, 39252, 1109, 40642]\n",
      "9       s0  07_209.xml             0   [34485, 39252, 1109, 40642, 42512]\n",
      "10      s1  07_209.xml             0      [812, 9467, 2100, 42512, 30971]\n",
      "11      s1  07_209.xml             0    [9467, 2100, 42512, 30971, 23350]\n",
      "12      s1  07_209.xml             0   [2100, 42512, 30971, 23350, 42512]\n",
      "13      s1  07_209.xml             0  [42512, 30971, 23350, 42512, 42512]\n",
      "14      s2  07_209.xml             0  [30971, 26965, 42512, 23092, 27288]\n",
      "15      s2  07_209.xml             0   [26965, 42512, 23092, 27288, 9330]\n",
      "16      s2  07_209.xml             0   [42512, 23092, 27288, 9330, 35705]\n",
      "17      s2  07_209.xml             1   [23092, 27288, 9330, 35705, 15027]\n",
      "18      s2  07_209.xml             0   [27288, 9330, 35705, 15027, 13945]\n",
      "19      s2  07_209.xml             0   [9330, 35705, 15027, 13945, 31900]\n",
      "20      s2  07_209.xml             0  [35705, 15027, 13945, 31900, 42512]\n",
      "21      s2  07_209.xml             0  [15027, 13945, 31900, 42512, 42512]\n",
      "22      s2  07_209.xml             0  [13945, 31900, 42512, 42512, 42512]\n",
      "23      s3  07_209.xml             0   [30971, 9555, 42512, 23092, 13945]\n",
      "24      s3  07_209.xml             0   [9555, 42512, 23092, 13945, 17550]\n",
      "25      s3  07_209.xml             0   [42512, 23092, 13945, 17550, 1109]\n",
      "26      s3  07_209.xml             0   [23092, 13945, 17550, 1109, 27288]\n",
      "27      s3  07_209.xml             0   [13945, 17550, 1109, 27288, 21789]\n",
      "28      s3  07_209.xml             0   [17550, 1109, 27288, 21789, 15027]\n",
      "29      s3  07_209.xml             0   [1109, 27288, 21789, 15027, 18346]\n",
      "...    ...         ...           ...                                  ...\n",
      "2028  s120  07_209.xml             0  [15027, 34802, 33631, 10107, 16240]\n",
      "2029  s120  07_209.xml             1  [34802, 33631, 10107, 16240, 34340]\n",
      "2030  s120  07_209.xml             0  [33631, 10107, 16240, 34340, 10813]\n",
      "2031  s120  07_209.xml             0   [10107, 16240, 34340, 10813, 4359]\n",
      "2032  s120  07_209.xml             0   [16240, 34340, 10813, 4359, 42512]\n",
      "2033  s121  07_209.xml             0   [4007, 13826, 40351, 27288, 30400]\n",
      "2034  s121  07_209.xml             0  [13826, 40351, 27288, 30400, 42512]\n",
      "2035  s121  07_209.xml             0  [40351, 27288, 30400, 42512, 42512]\n",
      "2036  s121  07_209.xml             0  [27288, 30400, 42512, 42512, 28758]\n",
      "2037  s121  07_209.xml             0  [30400, 42512, 42512, 28758, 36792]\n",
      "2038  s121  07_209.xml             0  [42512, 42512, 28758, 36792, 24620]\n",
      "2039  s121  07_209.xml             0  [42512, 28758, 36792, 24620, 13945]\n",
      "2040  s121  07_209.xml             0  [28758, 36792, 24620, 13945, 40945]\n",
      "2041  s121  07_209.xml             0   [36792, 24620, 13945, 40945, 4020]\n",
      "2042  s121  07_209.xml             0    [24620, 13945, 40945, 4020, 1109]\n",
      "2043  s121  07_209.xml             0    [13945, 40945, 4020, 1109, 27288]\n",
      "2044  s121  07_209.xml             0     [40945, 4020, 1109, 27288, 7504]\n",
      "2045  s121  07_209.xml             0     [4020, 1109, 27288, 7504, 15027]\n",
      "2046  s121  07_209.xml             0    [1109, 27288, 7504, 15027, 16602]\n",
      "2047  s121  07_209.xml             0   [27288, 7504, 15027, 16602, 30983]\n",
      "2048  s121  07_209.xml             0    [7504, 15027, 16602, 30983, 1109]\n",
      "2049  s121  07_209.xml             0   [15027, 16602, 30983, 1109, 27288]\n",
      "2050  s121  07_209.xml             0   [16602, 30983, 1109, 27288, 42512]\n",
      "2051  s121  07_209.xml             0   [30983, 1109, 27288, 42512, 23701]\n",
      "2052  s121  07_209.xml             0   [1109, 27288, 42512, 23701, 42512]\n",
      "2053  s122  07_209.xml             0  [42512, 10783, 42512, 13324, 27288]\n",
      "2054  s122  07_209.xml             0   [10783, 42512, 13324, 27288, 7501]\n",
      "2055  s122  07_209.xml             0   [42512, 13324, 27288, 7501, 42118]\n",
      "2056  s122  07_209.xml             0    [13324, 27288, 7501, 42118, 2100]\n",
      "2057  s122  07_209.xml             0    [27288, 7501, 42118, 2100, 42512]\n",
      "\n",
      "[2058 rows x 4 columns], [\"application for leave to appeal from 'show cause' dismissal by federal magistrate under r 44.12(1)(a) federal magistrates court rules\", 'whether failure to comply with the migration act 1958 (cth) ss 424a and 441a', 'whether breach of natural justice', 'whether improper exercise of power', 'whether failure to consider evidence favourable to applicant', 'whether bias.', 'migration'])\n",
      "('time-taken:', 0.44492602348327637)\n"
     ]
    }
   ],
   "source": [
    "sentences,catchphrases = get_statements(files[0])\n",
    "start=time.time()\n",
    "sentences, catch_words = fp.get_dataframe(pd.DataFrame(sentences),pd.DataFrame(catchphrases))\n",
    "print(sentences,catch_words)\n",
    "print(\"time-taken:\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_index_matrices(sentence_indexes):\n",
    "    length=len(sentence_indexes);\n",
    "    \n",
    "    sentence_indexes=[word_index['unk'],word_index['unk']]+sentence_indexes+[word_index['unk'],word_index['unk']];    \n",
    "    out_indices=[]\n",
    "    for i in range(2,length):\n",
    "        out_indices.append(sentence_indexes[i-2:i+3]);\n",
    "        \n",
    "    return out_indices;\n",
    "\n",
    "def prepare_label_matrices(labels):\n",
    "    length=len(labels);\n",
    "    labels=[0,0]+labels+[0,0];\n",
    "    out_labels=[];\n",
    "    \n",
    "    for i in range(2,length):\n",
    "        out_labels.append(labels[i]);\n",
    "        \n",
    "    return out_labels;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_indexes_labels(sentences,catchphrases):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    labels=[];\n",
    "    \n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "            \n",
    "    for word in sentences:\n",
    "        if word in catchphrases:\n",
    "            labels.append(1.0);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            labels.append(0.0);\n",
    "            \n",
    "    return sentence_indexes,labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_sentences(file_name):\n",
    "    temp_data=full_data[(full_data.file_id==file_name) & full_data['Id'].str.startswith(('s'))]\n",
    "    return list(temp_data.text.values)[:-1]\n",
    "\n",
    "\n",
    "def get_phrases( phrase_indices_2, output):\n",
    "    catch_phrases_temp = []\n",
    "    for i, out in enumerate(output):\n",
    "        if out == 1:\n",
    "            phrase = ''\n",
    "            for ind in range(5):\n",
    "                phrase += reverseWordIndex[phrase_indices_2[i][ind]]+\" \";\n",
    "                catch_phrases_temp.append(phrase[:-1])\n",
    "    return catch_phrases_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def get_phrases_scores(true_list, predicted_list):\n",
    "    from collections import defaultdict\n",
    "    scores_map = defaultdict(float)\n",
    "    for phrase in predicted_list:\n",
    "        max_prec=0;\n",
    "        for true_phrase in true_list:\n",
    "            [precision, recall, f_score] = rouge.rouge_l(phrase, true_phrase);\n",
    "            if precision>max_prec:\n",
    "                max_prec=precision;\n",
    "        scores_map[phrase]=max_prec;\n",
    "    return scores_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(1, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def MLP( stacked_tensor, weights_input, weights_input_2, bias1, bias2):\n",
    "    h1 = (tf.matmul(weights_input,stacked_tensor,transpose_a=True,transpose_b=True))\n",
    "    #h1 = tf.add(h1, bias1)\n",
    "    h1 = tf.nn.tanh(h1)\n",
    "    h2 = tf.matmul(weights_input_2,h1,transpose_a=True,transpose_b=False)\n",
    "    #h2 = tf.add(h2, bias2)\n",
    "    h2 = tf.nn.sigmoid(h2)\n",
    "    #h2 = tf.round(tf.transpose(h2))\n",
    "    return tf.transpose(h2)\n",
    "    #return tf.cast(tf.to_float(h2>0.5),dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    \n",
    "## Building Graph\n",
    "phrases_indices = tf.placeholder(tf.int32, shape=[None, 5])\n",
    "phrase_labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "test_phrases_indices = tf.placeholder(tf.int32, shape=[None, 5])\n",
    "\n",
    "W = tf.Variable(embedding_array, dtype=tf.float32,name=\"W\")\n",
    "#W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "phrase_tensor = tf.nn.embedding_lookup(W, phrases_indices)\n",
    "\n",
    "#sentence_indices = tf.placeholder(tf.int32, shape=[None,1])\n",
    "#sentence_embeddings = tf.placeholder(tf.float32, shape=[None,200])\n",
    "#sentence_tensor = tf.nn.embedding_lookup(sentence_embeddings, sentence_indices)\n",
    "#sentence_tensor=tf.reshape(sentence_tensor, [tf.shape(sentence_tensor)[0], 200])\n",
    "\n",
    "#doc_embedding = tf.placeholder(tf.float32, shape=[1,embedding_size])\n",
    "#document_tensor = tf.tile(doc_embedding, [tf.shape(phrases_indices)[0], 1])\n",
    "\n",
    "phrase_tensor = tf.reshape(phrase_tensor, [tf.shape(phrase_tensor)[0], embedding_size*5])\n",
    "conv_filter = tf.Variable(tf.truncated_normal((embedding_size*5, embedding_size), stddev=0.1))\n",
    "word_tensor = tf.nn.relu(tf.matmul(phrase_tensor, conv_filter))\n",
    "\n",
    "#stacked_tensor = tf.concat(values=[word_tensor, sentence_tensor, document_tensor], axis=1)\n",
    "#stacked_tensor = tf.concat(values=[word_tensor, document_tensor], axis=1)\n",
    "stacked_tensor = tf.concat(values=[word_tensor], axis=1)\n",
    "weights_input = tf.Variable(tf.random_normal((embedding_size*1, cnn_filter_size), stddev=0.2))\n",
    "weights_input_2 = tf.Variable(tf.random_normal((100, 1), stddev=0.2))\n",
    "\n",
    "bias1 = tf.Variable(tf.random_normal((1, cnn_filter_size), stddev=0.2))\n",
    "bias2 = tf.Variable(tf.random_normal((1, 1), stddev=0.2))\n",
    "print(bias2)\n",
    "\n",
    "sentence_values=MLP(stacked_tensor, weights_input, weights_input_2, bias1, bias2)\n",
    "\n",
    "#prediction_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=phrase_labels,logits=sentence_values)\n",
    "loss = tf.reduce_mean(tf.keras.metrics.mean_squared_error(phrase_labels, sentence_values))\n",
    "#loss = tf.convert_to_tensor(tf.reduce_mean(sentence_values));\n",
    "\n",
    "#using the gradient descent optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "#clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "app = optimizer.apply_gradients(grads)\n",
    "\n",
    "#For test data\n",
    "test_embed = tf.nn.embedding_lookup(W, test_phrases_indices)\n",
    "test_embed = tf.reshape(test_embed, [tf.shape(test_embed)[0], embedding_size*5])\n",
    "test_conv_filter = tf.Variable(tf.truncated_normal((embedding_size*5, embedding_size), stddev=0.1));\n",
    "\n",
    "test_word_tensor = tf.nn.relu(tf.matmul(test_embed, test_conv_filter));\n",
    "test_pred = MLP(test_word_tensor, weights_input, weights_input_2, bias1, bias2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train( sess, num_steps):\n",
    "    init.run()\n",
    "    #print(\"Initailized\")\n",
    "    start=time.time()\n",
    "    total_loss=0.0;\n",
    "    train_files = files_2006 + files_2007 + files_2008\n",
    "    for file in train_files:\n",
    "        start=time.time()\n",
    "        file_data_frame = None; catch_words = None\n",
    "        try:\n",
    "            sentences, catchphrases=get_statements(file);\n",
    "            file_data_frame, catch_words = fp.get_dataframe(pd.DataFrame(sentences),pd.DataFrame(catchphrases))\n",
    "        except:\n",
    "            print('************************BAD DATA IN FILE, file=', file)\n",
    "            continue\n",
    "        #needs sentences list\n",
    "        #sentences_list = fp.get_sentences(file) #indexes/order preserved\n",
    "        #sentence_embeddings = sen2vec(sentences_list)\n",
    "        #sentence_embeddings = get_sent_embeddings(sentences_list)\n",
    "        #sentence_ids_to_indices = np.array([int(x[1:]) for x in list(file_data_frame['Id'].values)])\n",
    "\n",
    "        #phrase inputs and labels\n",
    "        phrases_indices_1 = np.array(list(file_data_frame['words'].values))\n",
    "        phrases_label_matrix = np.array(list(file_data_frame['is_catchword'].values))\n",
    "\n",
    "        #doc embedding\n",
    "        #doc_embedding = d2v_model.docvecs[file]\n",
    "        #doc_embedding = np.reshape(doc_embedding, (1, embedding_size))\n",
    "        #sentence_ids_to_indices = np.reshape(sentence_ids_to_indices, (sentence_ids_to_indices.shape[0], 1))\n",
    "        phrases_label_matrix = np.reshape(phrases_label_matrix, (phrases_label_matrix.shape[0], 1))\n",
    "\n",
    "        feed_dict = {#self.doc_embedding: doc_embedding, \n",
    "                     phrases_indices: phrases_indices_1, \n",
    "                     phrase_labels: phrases_label_matrix}\n",
    "        \n",
    "        #loss_val = sess.run([self.loss], feed_dict=feed_dict)  \n",
    "        _,loss_val,outputs = sess.run([app,loss,sentence_values], feed_dict=feed_dict)\n",
    "\n",
    "        if num_steps%20 == 0:\n",
    "            print('Train Finished for file ', file, ' loss_val=', loss_val)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def test( sess, file):\n",
    "    try:\n",
    "        sentences, catchphrases = get_statements(file);\n",
    "        file_data_frame, catch_words = fp.get_dataframe(pd.DataFrame(sentences),pd.DataFrame(catchphrases))\n",
    "    except:\n",
    "        print('************************BAD DATA IN FILE, file=', file)\n",
    "        return {}\n",
    "\n",
    "    test_phrases_indices_1 = np.array(list(file_data_frame['words'].values))\n",
    "    phrases_label_matrix = np.array(list(file_data_frame['is_catchword'].values))\n",
    "    phrases_label_matrix = np.reshape(phrases_label_matrix, (phrases_label_matrix.shape[0], 1))\n",
    "\n",
    "    feed_dict = {test_phrases_indices: test_phrases_indices_1}\n",
    "    outputs = sess.run([test_pred], feed_dict=feed_dict)\n",
    "    \n",
    "    outputs = [1 if x[0]>=threshold else 0 for x in outputs[0].tolist()] \n",
    "\n",
    "    predicted_catch_phrases = get_phrases(test_phrases_indices_1, outputs)\n",
    "    print(\"Reched end of testing\")\n",
    "    return get_phrases_scores(catchphrases, predicted_catch_phrases)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_embeddings(sess,embedding_array):\n",
    "    sess.run(W, feed_dict={W: np.asarray(embedding_array)})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(inputs):\n",
    "    out=np.zeros((len(inputs),embedding_size));\n",
    "    for i in range(len(inputs)):\n",
    "        try:\n",
    "            c = w2v[inputs[i]]\n",
    "        except KeyError:\n",
    "            c = np.zeros((1,embedding_size));\n",
    "        out[i]=c;\n",
    "    return np.transpose(out);\n",
    "\n",
    "\n",
    "def get_precision(result):\n",
    "    mean_Res=0.0\n",
    "    count=1;\n",
    "    for v in result:\n",
    "        if not  math.isnan(v):\n",
    "            mean_Res += v\n",
    "            count += 1\n",
    "        else:\n",
    "            print(v);\n",
    "    print(mean_Res/count)\n",
    "\n",
    "    \n",
    "def get_key_phrases(phrases):\n",
    "    res_map = {}\n",
    "    for phrase in phrases.keys():\n",
    "        if 'unk' not in phrase:\n",
    "            res_map[phrase] = phrases[phrase];\n",
    "    return res_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train Finished for file ', '06_159.xml', ' loss_val=', 0.22676311)\n",
      "('**************************Train_Time = epoch', 0.19909286499023438, '  mins: ', 0.0)\n",
      "Model saved in path: ./MyModels/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "### Training module\n",
    "\n",
    "with tf.Session().as_default() as sess:\n",
    "    load_embeddings(sess,embedding_array);\n",
    "    start = time.time()\n",
    "    for i in range(1):\n",
    "        train(sess, i)\n",
    "    t=time.time()-start\n",
    "    print ('**************************Train_Time = epoch', t, '  mins: ',divmod(divmod(t, 3600)[1], 60)[0])\n",
    "\n",
    "    \n",
    "### Saving the model\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"./MyModels/model.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./MyModels/model.ckpt\n",
      "Saved model restored\n",
      "Reched end of testing\n",
      "0.952380952381\n",
      "('Testing done for file: ', '09_245.xml', ' MeanPrecision: ', None)\n",
      "('Output Phrases: ', ['detailed'])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Loading from saved model\n",
    "\n",
    "files_phrase_score_map = {}\n",
    "\n",
    "with tf.Session() as new_sess:\n",
    "    load_embeddings(new_sess,embedding_array);\n",
    "    saver.restore(new_sess, \"./MyModels/model.ckpt\")\n",
    "    print(\"Saved model restored\")\n",
    "\n",
    "    \n",
    "    test_files = files_2009\n",
    "    \n",
    "    \n",
    "    ### TO test more files, increasing number in below test_files's slice \n",
    "    for file in test_files[:1]:\n",
    "        phrase_score_map = test(new_sess, file)\n",
    "        files_phrase_score_map[file] = phrase_score_map\n",
    "        \n",
    "        predictions = get_key_phrases(phrase_score_map)        \n",
    "        top_k =  sorted(predictions.values(), reverse = True)[:20]\n",
    "        res = set([])\n",
    "        for k in top_k:\n",
    "            res.add(predictions.keys()[predictions.values().index(k)])\n",
    "\n",
    "        print ('Testing done for file: ', file, ' MeanPrecision: ', get_precision(top_k))\n",
    "        print ('Output Phrases: ', list(res))\n",
    "        print ('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
