{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(string):\n",
    "    tokens = string.split()\n",
    "    n_tokens = len(tokens)\n",
    "    return n_tokens  \n",
    "\n",
    "def ReLU(x):\n",
    "    return abs(x) * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_row(file_id,Id,text):\n",
    "    text=text.lower();\n",
    "    window_size = 2\n",
    "    word_count=get_word_count(text);\n",
    "    words=text.split()\n",
    "    input_filter=np.random.rand(300,200*(2*window_size+1));\n",
    "    sentence_vector=np.zeros((300,1));\n",
    "    count=0;\n",
    "    rows_list=[]\n",
    "    if word_count>2*window_size+1:\n",
    "        for i in range(window_size,word_count-window_size):\n",
    "            \n",
    "            inputs=words[i-window_size:i+window_size+1];\n",
    "            inputs=lookup_indexes(inputs)\n",
    "            #print(inputs);\n",
    "            #input_features=ReLU(get_features(inputs));\n",
    "            #input_features=np.reshape(input_features,(200*(2*window_size+1),1));\n",
    "            #input_features=ReLU(np.matmul(input_filter,input_features));\n",
    "            #print(np.shape(input_features));\n",
    "            rows_list.append({\"file_id\":file_id,\"Id\":Id,\"words\":inputs})\n",
    "            #print(file_id,Id,inputs);\n",
    "            #sentence_vector=np.add(sentence_vector,input_features);\n",
    "            #count=count+1;\n",
    "        #sentence_vector= np.true_divide(sentence_vector, 4);\n",
    "        #print(np.shape(sentence_vector));\n",
    "    #print(rows_list)\n",
    "    child_dataframe=pd.DataFrame(rows_list);\n",
    "    #print(child_dataframe.head(5))\n",
    "    return child_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1795, 5)\n",
      "('total time:', 5.125476121902466)\n",
      "   Id      file_id                     words\n",
      "0  s0  08_1056.xml  [176, 13, 759, 396, 584]\n",
      "1  s0  08_1056.xml    [13, 759, 396, 584, 0]\n",
      "2  s0  08_1056.xml  [759, 396, 584, 0, 3250]\n",
      "3  s0  08_1056.xml  [396, 584, 0, 3250, 845]\n",
      "4  s0  08_1056.xml   [584, 0, 3250, 845, 21]\n"
     ]
    }
   ],
   "source": [
    "def lookup_indexes(sentences):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    \n",
    "    \n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "        \n",
    "    return sentence_indexes;\n",
    "\n",
    "full_data=pd.read_csv('./testdata.csv');\n",
    "print(np.shape(full_data))\n",
    "\n",
    "catch_df=full_data[full_data['Id'].str.startswith(('c'))]\n",
    "sent_df=full_data[full_data['Id'].str.startswith(('s'))]\n",
    "\n",
    "start=time.time();\n",
    "words_dataframes=[];\n",
    "\n",
    "for index, row in sent_df.iterrows():\n",
    "    words_dataframes.append(get_features_row(row['file_id'],row['Id'],row['text']))\n",
    "end=time.time();\n",
    "\n",
    "words_df=pd.concat(words_dataframes,ignore_index=True)\n",
    "\n",
    "print(\"total time:\",end-start);\n",
    "\n",
    "print(words_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    dvocab, w2v,word_index  = read_data(filename);\n",
    "    return w2v,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Words in DataSet:', 400000)\n",
      "Loaded embeddings\n",
      "reading data completed\n",
      "201534\n"
     ]
    }
   ],
   "source": [
    "embedding_filename = \"glove_6B_200d.txt\"\n",
    "embedding_array,word_index = load_embeddings(embedding_filename)\n",
    "print(\"Loaded embeddings\")\n",
    "\n",
    "words_data=pd.read_csv(\"words_data.csv\");\n",
    "catchphrases_data=pd.read_csv(\"catchwords_data.csv\")\n",
    "print(\"reading data completed\");\n",
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00822406,  0.0158696 ,  0.01852092, ..., -0.04450814,\n",
       "        -0.02824019, -0.06522587],\n",
       "       [ 0.08001935,  0.02381717, -0.01781278, ...,  0.03001026,\n",
       "        -0.07005218, -0.0089276 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent2Vec(sentences):\n",
    "    embed = hub.Module(module_url)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        sentenceEmbeddings = session.run(embed(sentences))\n",
    "        \n",
    "        return sentenceEmbeddings\n",
    "        #return \n",
    "\n",
    "sent2Vec(['UWA was ordered to pay the costs of Dr Gra','Sirtex succeeded in its cross-claim against Dr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id      file_id                     words\n",
      "0  s0  08_1056.xml  [176, 13, 759, 396, 584]\n",
      "1  s0  08_1056.xml    [13, 759, 396, 584, 0]\n",
      "2  s0  08_1056.xml  [759, 396, 584, 0, 3250]\n",
      "3  s0  08_1056.xml  [396, 584, 0, 3250, 845]\n",
      "4  s0  08_1056.xml   [584, 0, 3250, 845, 21]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <type 'function'> to Tensor. Contents: <function sent2Vec at 0x7feb6b789c80>. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f6ea8df54e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"1.xml\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f6ea8df54e95>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(sentenceIndices, docVector, wordDf)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#wordDf [(sentence_id,doc_id, [word-2,word-1,word,word+1,word+2])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentenceIndicesTf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceIndices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentEmbeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentenceIndicesTf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# n * 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/embedding_ops.pyc\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/embedding_ops.pyc\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    127\u001b[0m     if not any(\n\u001b[1;32m    128\u001b[0m         isinstance(p, resource_variable_ops.ResourceVariable) for p in params):\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_n_to_tensor_or_indexed_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_n_to_tensor_or_indexed_slices\u001b[0;34m(values, dtype, name)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \"\"\"\n\u001b[1;32m   1374\u001b[0m   return internal_convert_n_to_tensor_or_indexed_slices(\n\u001b[0;32m-> 1375\u001b[0;31m       values=values, dtype=dtype, name=name, as_ref=False)\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_n_to_tensor_or_indexed_slices\u001b[0;34m(values, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1344\u001b[0m       ret.append(\n\u001b[1;32m   1345\u001b[0m           internal_convert_to_tensor_or_indexed_slices(\n\u001b[0;32m-> 1346\u001b[0;31m               value, dtype=dtype, name=n, as_ref=as_ref))\n\u001b[0m\u001b[1;32m   1347\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor_or_indexed_slices\u001b[0;34m(value, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1303\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     return internal_convert_to_tensor(\n\u001b[0;32m-> 1305\u001b[0;31m         value, dtype=dtype, name=name, as_ref=as_ref)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                          as_ref=False):\n\u001b[1;32m    227\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    205\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    206\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 207\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    208\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    529\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    530\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <type 'function'> to Tensor. Contents: <function sent2Vec at 0x7feb6b789c80>. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "def driver(sentenceIndices, docVector, wordDf):\n",
    "    #for each document\n",
    "    #sentenceDf [s1_id,s2_id,s3_id.....]\n",
    "    #docVector [1x200]\n",
    "    #wordDf [(sentence_id,doc_id, [word-2,word-1,word,word+1,word+2])]\n",
    "    sentenceIndicesTf = tf.convert_to_tensor(sentenceIndices)\n",
    "    sentEmbeddings = tf.nn.embedding_lookup(sent2Vec, sentenceIndicesTf)\n",
    "    # n * 200\n",
    "    \n",
    "#     wordIndicesTf = tf.convert_to_tensor(wordIndeces)\n",
    "#     wordEmbed = tf.nn.embedding_lookup(word2Vec, wordIndicesTf)\n",
    "    fW = np.array([None,1000])\n",
    "    i=0\n",
    "    for i in range(len(wordDf)):\n",
    "        import pdb; pdb.set_trace()\n",
    "        wordIndices = wordDf.loc[i]['words']\n",
    "        fW[i] = np.stack(tuple([embedding_array[word_index[x]] for x in wordIndices]))\n",
    "        i += 1\n",
    "            \n",
    "#     W = tf.Variable(tf.constant(0.0, shape=[1000, 200 ]),trainable=True, name=\"W\")\n",
    "#     embedding_placeholder = tf.placeholder(tf.float32, [1000, 200])\n",
    "#     embedding_init = W.assign(self.embedding_placeholder)\n",
    "    \n",
    "#     #(n*1000)*(1000*200) -> (n*200)\n",
    "#     wordEmbeddings = tf.matmul(wordEmbed, embedding_init)\n",
    "    \n",
    "    \n",
    "#     docPlaceHolder = tf.placeholder(tf.int32, shape=[None,200])\n",
    "#     docEnbeddings = tf.nn.embedding_lookup(docVector, docPlaceHolder)\n",
    "\n",
    "    \n",
    "# import pdb; pdb.set_trace()\n",
    "print(words_df.head())\n",
    "    \n",
    "driver([0,1],\"1.xml\",words_df.head())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
