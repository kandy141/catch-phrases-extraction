{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Config\n",
    "import os\n",
    "from os.path import isfile,join\n",
    "import xml.etree.cElementTree as ET\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "#import doc2vec_impl\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size=10\n",
    "embedding_filename = \"glove_6B_200d.txt\"\n",
    "training_epochs=1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"corpus/fulltext/\"\n",
    "files = [f for f in os.listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.9957069  -1.7302711  -1.2416555  -1.25041    -0.8760755   1.4730991\n",
      "  1.2059473   0.38232905 -1.6187527   1.305874    1.1770602   2.308416\n",
      "  0.02343252 -0.2502587   1.7815354  -1.332049    0.38943136 -2.052053\n",
      " -1.3515872  -1.5760187  -0.59295475 -0.66141963  1.3531227   0.47351202\n",
      "  0.80553395  2.9402657   2.2971811   0.9491949   4.030078    0.6117527\n",
      " -0.27292827 -1.1015353  -0.3637588  -1.0808604  -0.38145977 -2.5174286\n",
      "  1.1072197   2.530051    2.5857248  -1.1959172   1.7793411   0.09467691\n",
      " -1.4371932  -2.9791405   1.0630684   0.9067003   0.43923157  3.4134736\n",
      "  0.47711533 -1.3631905  -3.3734288  -0.2367886   0.3390342  -0.39412883\n",
      "  0.5333661   0.7227717   0.55354226  0.35878623  0.932402    2.4869206\n",
      " -0.55152404  2.4435148  -0.3364468  -1.9685099  -1.7815394  -0.6749915\n",
      "  2.4418046  -2.3903039   1.1918342   0.36871168 -1.7374214   0.7597355\n",
      "  1.6034806  -0.9551855   0.8539238  -1.5502286   0.8968359  -0.62780535\n",
      " -1.7566789   4.206336    0.92475533  0.4445385   1.4446664   0.18256582\n",
      "  0.33426532 -1.4212458   2.6209822   2.3302562   0.24014099  1.7833108\n",
      "  1.2331686   0.23349828 -3.6794689  -2.0452528  -0.9031459   0.8323558\n",
      " -0.8843479   0.30374396  0.87310374  3.1438227   1.6313096   1.6776122\n",
      "  0.48421437 -4.023881    3.1790373  -0.36640164 -1.5662786   2.1223414\n",
      "  2.1331837  -2.4567196   0.45546478  0.5395071  -1.0458131  -1.7282628\n",
      " -0.5866408  -0.7538245  -1.1013936   0.7281821  -0.9490656  -0.9584874\n",
      "  0.03865002  1.0363113   3.701328    1.6680748  -1.9616818   0.02108546\n",
      "  0.06672078  1.3992096  -1.1087863   0.8012173   1.5391992   0.5408178\n",
      "  0.942101   -0.39909512  0.17921649 -1.1100775   1.6705716   0.8876011\n",
      " -2.5764427   0.540361    0.12292239  1.3356769  -0.2826225  -0.77397394\n",
      " -1.3675452  -1.1290836  -2.5003982   2.3073096  -0.05056152 -1.0492153\n",
      " -0.10702924 -1.2159368   0.846608   -0.41358626 -0.83953726 -1.1248862\n",
      " -1.0842489  -0.39818436  1.2597241  -0.7062385  -3.208376   -1.7201724\n",
      " -0.8565359   0.6638217  -1.3636007  -3.1470718  -1.5880749   0.38425222\n",
      "  0.07932217  0.9742497   1.4434265   1.7713523  -1.6882014   1.6205076\n",
      " -4.433126   -1.5319406  -3.1952329  -1.1040304  -0.28445873  0.5716109\n",
      " -2.9582093   3.44765     2.682068   -1.5100415   0.10066229 -0.9557712\n",
      "  0.14830023  1.6512151  -0.39719307 -2.6286154  -1.2253991  -0.263822\n",
      " -2.4644768   0.3337611  -0.40415186  2.0325894  -1.2030413   0.39076445\n",
      " -0.7095663  -1.9799259 ]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list=set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "##########loading dev2vec=============\n",
    "#doc2vec_impl.main();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('doc2vec_models/doc2vec.model')\n",
    "#start testing\n",
    "#printing the vector of document at index 1 in docLabels\n",
    "docvec = d2v_model.docvecs[files[0]]\n",
    "print(docvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    dvocab, w2v,word_index  = read_data(filename);\n",
    "    return w2v,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataSet: 400000\n",
      "Loaded embeddings\n",
      "reading data completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_array,word_index = load_embeddings(embedding_filename)\n",
    "print(\"Loaded embeddings\")\n",
    "\n",
    "words_data=pd.read_csv(\"words_data.csv\");\n",
    "catchphrases_data=pd.read_csv(\"catch_data.csv\")\n",
    "print(\"reading data completed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201534\n"
     ]
    }
   ],
   "source": [
    "print(word_index['unk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(file):\n",
    "    with open(dirpath + file, 'r',encoding=\"utf-8\", errors='replace') as f:\n",
    "        data=str(f.read());\n",
    "        data=data.lower()\n",
    "        data = data.replace(\"\\\"id=\", \"id=\\\"\");\n",
    "        data=data.replace(\"\\n\",\"\")\n",
    "        data=data.replace('\".*?=.*?\"', \"\",)\n",
    "        data=data.replace(\"&\",\"\");\n",
    "        xml = ET.fromstring(str(data))\n",
    "        name=None;\n",
    "        rows_list=[];\n",
    "        catchphrases=[];\n",
    "        sentences=[];\n",
    "        for child in xml:\n",
    "            if child.tag==\"catchphrases\":\n",
    "                for catchphrase in child:\n",
    "                    id=catchphrase.attrib.get(\"id\")\n",
    "                    #print(catchphrase.text)\n",
    "                    #catchphrases.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":catchphrase.text})\n",
    "                    catchphrases+=tokenizer.tokenize(catchphrase.text)\n",
    "            if child.tag==\"sentences\":\n",
    "                for sentence in child:\n",
    "                    id = sentence.attrib.get(\"id\")\n",
    "                    #sentences.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":sentence.text})\n",
    "                    sentences+=tokenizer.tokenize(sentence.text)\n",
    "    \n",
    "    \n",
    "    sentences = [w for w in sentences if not w in stop_words_list] \n",
    "    catchphrases = [w for w in catchphrases if not w in stop_words_list]\n",
    "    \n",
    "    \n",
    "    #sentences=[word.lower() for word in sentences if word.isalphanum()]\n",
    "    #catchphrases=[word.lower() for word in catchphrases if word.isalphanum()]\n",
    "    \n",
    "    return sentences,catchphrases\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_index_matrices(sentence_indexes):\n",
    "    length=len(sentence_indexes);\n",
    "    \n",
    "    sentence_indexes=[word_index['unk'],word_index['unk']]+sentence_indexes+[word_index['unk'],word_index['unk']];    \n",
    "    out_indices=[]\n",
    "    for i in range(2,length):\n",
    "        out_indices.append(sentence_indexes[i-2:i+3]);\n",
    "        \n",
    "    return out_indices;\n",
    "\n",
    "def prepare_label_matrices(labels):\n",
    "    length=len(labels);\n",
    "    labels=[0,0]+labels+[0,0];\n",
    "    out_labels=[];\n",
    "    \n",
    "    for i in range(2,length):\n",
    "        out_labels.append(labels[i]);\n",
    "        \n",
    "    return out_labels;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_indexes_labels(sentences,catchphrases):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    labels=[];\n",
    "    \n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "            \n",
    "    for word in sentences:\n",
    "        if word in catchphrases:\n",
    "            labels.append(1.0);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            labels.append(0.0);\n",
    "            \n",
    "    return sentence_indexes,labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2966 2966\n",
      "2964 2964\n"
     ]
    }
   ],
   "source": [
    "sentences,catchphrases=get_statements(files[0]);\n",
    "sentence_indexes,labels=lookup_indexes_labels(sentences,catchphrases)\n",
    "print(len(sentence_indexes),len(labels))\n",
    "\n",
    "sentence_index_matrix=prepare_index_matrices(sentence_indexes);\n",
    "label_matrix=prepare_label_matrices(labels);\n",
    "\n",
    "print(len(sentence_index_matrix),len(label_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CatchPhraseExtractor(object):\n",
    "\n",
    "    def __init__(self, graph, embedding_array):\n",
    "        self.total_loss=0.0\n",
    "        self.build_graph(graph, embedding_array)\n",
    "\n",
    "    def build_graph(self, graph, embedding_array):\n",
    "        print(\"generating graph\");\n",
    "\n",
    "        with graph.as_default():\n",
    "\n",
    "            #self.embeddings = tf.Variable(embedding_array, dtype=tf.float32);\n",
    "            W = tf.Variable(tf.constant(0.0, shape=[400000, 200 ]),trainable=True, name=\"W\")\n",
    "            self.embedding_placeholder = tf.placeholder(tf.float32, [400000, 200])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[None,5])\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            \n",
    "            sentence_vectors=tf.nn.embedding_lookup(W, self.train_inputs)\n",
    "            sentence_vectors = tf.reshape(sentence_vectors, [tf.shape(sentence_vectors)[0], 1000])\n",
    "    \n",
    "            \n",
    "            conv_filter=tf.Variable(tf.truncated_normal((1000,200), stddev=0.1));\n",
    "            word_features=tf.nn.relu(tf.matmul(sentence_vectors, conv_filter));\n",
    "            \n",
    "            doc_vector=tf.reduce_max(word_features,reduction_indices=[0],keepdims=True);\n",
    "            print(doc_vector)\n",
    "            \n",
    "            sentence_doc_dummy=tf.tile(doc_vector, [tf.shape(word_features)[0], 1])\n",
    "            sentence_tensor = tf.reshape(tf.concat([word_features, sentence_doc_dummy],1),[tf.shape(word_features)[0],400])\n",
    "\n",
    "            weights_input=tf.Variable(tf.random_normal((400,100), stddev=0.2));\n",
    "            weights_input_2=tf.Variable(tf.random_normal((100,1), stddev=0.2));\n",
    "            \n",
    "            #weights_input=tf.Variable(tf.ones((400,200)));\n",
    "            #weights_input_2=tf.Variable(tf.ones((200,1)));\n",
    "            \n",
    "            bias1=tf.Variable(tf.random_normal((100,1), stddev=0.2));\n",
    "            bias2=tf.Variable(tf.random_normal((1,1), stddev=0.2));\n",
    "            \n",
    "            self.sentence_values=self.MLP(sentence_tensor,weights_input,weights_input_2,bias1,bias2);\n",
    "            \n",
    "            prediction_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.train_labels,logits=self.sentence_values)\n",
    "            \n",
    "            l2_input = tf.nn.l2_loss(weights_input)\n",
    "            l2_output = tf.nn.l2_loss(weights_input_2)\n",
    "\n",
    "            regularized_loss=l2_input + l2_output\n",
    "            regularized_loss=1e-1 * regularized_loss\n",
    "\n",
    "            #calculating the total loss:sum of prediction loss and regularized loss\n",
    "            total_loss = prediction_loss + regularized_loss\n",
    "            \n",
    "            self.loss=tf.reduce_mean(total_loss)\n",
    "            \n",
    "            #using the gradient descent optimizer\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            #clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(grads)\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    def MLP(self,x,weights_input,weights_input_2,bias1,bias2):\n",
    "        h1=(tf.matmul( weights_input,x,transpose_a=True,transpose_b=True));\n",
    "        h1=h1+bias1;\n",
    "        \n",
    "        h1=tf.nn.tanh(h1);\n",
    "        \n",
    "        h2=tf.matmul(weights_input_2,h1,transpose_a=True,transpose_b=False);\n",
    "        h2=h2+bias2;\n",
    "        h2=tf.nn.sigmoid(h2);\n",
    "        return tf.transpose(h2);\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self,total_loss):\n",
    "        total_loss=tf.convert_to_tensor(total_loss)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            print(variables)\n",
    "        return total_loss, tape.gradient(tf.convert_to_tensor(total_loss), variables)\n",
    "        \n",
    "    \n",
    "    def train(self, sess, num_steps):\n",
    "        \n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "        total_loss=0.0;\n",
    "        for file in files[:1]:\n",
    "            sentences,catchphrases=get_statements(file);\n",
    "            \n",
    "            \n",
    "            sentence_indexes,labels=lookup_indexes_labels(sentences,catchphrases)\n",
    "            \n",
    "            sentence_index_matrix=prepare_index_matrices(sentence_indexes);\n",
    "            label_matrix=prepare_label_matrices(labels);\n",
    "            \n",
    "            sentence_index_matrix=np.asarray(sentence_index_matrix)\n",
    "            label_matrix=np.asarray(label_matrix)\n",
    "        \n",
    "            #sentence_indexes=np.reshape(sentence_indexes,(sentence_indexes.shape[0],1))\n",
    "            label_matrix=np.reshape(label_matrix,(label_matrix.shape[0],1))\n",
    "            \n",
    "            \n",
    "            feed_dict = {self.train_inputs: sentence_index_matrix, self.train_labels: label_matrix}\n",
    "            #loss_val = sess.run([self.loss], feed_dict=feed_dict)  \n",
    "            _,loss_val,outputs = sess.run([self.app,self.loss,self.sentence_values], feed_dict=feed_dict)\n",
    "\n",
    "            for i in range(len(outputs)):\n",
    "                if label_matrix[i]==1.0:\n",
    "                    print(outputs[i],label_matrix[i])\n",
    "            print(\"Train Finished.\",loss_val)\n",
    "        \n",
    "    def load_embeddings(self,embedding_array):\n",
    "        sess.run(self.embedding_init, feed_dict={self.embedding_placeholder: np.asarray(list(embedding_array.values()))})\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(inputs):\n",
    "    out=np.zeros((len(inputs),200));\n",
    "    for i in range(len(inputs)):\n",
    "        try:\n",
    "            c = w2v[inputs[i]]\n",
    "        except KeyError:\n",
    "            c = np.zeros((1,200));\n",
    "        out[i]=c;\n",
    "    \n",
    "    return np.transpose(out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating graph\n",
      "Tensor(\"Max:0\", shape=(1, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "model = CatchPhraseExtractor(graph, embedding_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initailized\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "Train Finished. 81.37931\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "  \n",
    "    model.load_embeddings(embedding_array)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    for i in range(1):\n",
    "        model.train(sess, 10)   \n",
    "    #optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
