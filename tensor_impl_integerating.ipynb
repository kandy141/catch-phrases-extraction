{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-98b7f4ce9e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named Config"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile,join\n",
    "import xml.etree.cElementTree as ET\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "#import doc2vec_impl\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size=10\n",
    "embedding_filename = \"glove_6B_200d.txt\"\n",
    "training_epochs=1\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'corpus/fulltext/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9bd299c1d6d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"corpus/fulltext/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'corpus/fulltext/'"
     ]
    }
   ],
   "source": [
    "dirpath = \"corpus/fulltext/\"\n",
    "files = [f for f in os.listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d4692f2311b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##########loading dev2vec=============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#doc2vec_impl.main();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stop_words_list=set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "##########loading dev2vec=============\n",
    "#doc2vec_impl.main();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('doc2vec_models/doc2vec.model')\n",
    "#start testing\n",
    "#printing the vector of document at index 1 in docLabels\n",
    "docvec = d2v_model.docvecs[files[0]]\n",
    "print(docvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    dvocab, w2v,word_index  = read_data(filename);\n",
    "    return w2v,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_array,word_index = load_embeddings(embedding_filename)\n",
    "print(\"Loaded embeddings\")\n",
    "\n",
    "words_data=pd.read_csv(\"words_data.csv\");\n",
    "catchphrases_data=pd.read_csv(\"catch_data.csv\")\n",
    "print(\"reading data completed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/2'.\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 58.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 108.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 158.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 218.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 268.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 338.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 398.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 438.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 468.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 498.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 538.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 568.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 618.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 658.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 698.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 728.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 758.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 808.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 848.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 898.00MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 958.00MB\n",
      "INFO:tensorflow:Downloaded https://tfhub.dev/google/universal-sentence-encoder/2, Total size: 993.27MB\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/2'.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "##sen2vec embeddings\n",
    "def sen2Vec(sentences):\n",
    "    embed = hub.Module(module_url)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        sentenceEmbeddings = session.run(embed(sentences))    \n",
    "        return sentenceEmbeddings\n",
    "\n",
    "sentence_embeddings = sen2Vec(['UWA was ordered to pay the costs of Dr Gra','Sirtex succeeded in its cross-claim against Dr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index['unk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(file):\n",
    "    with open(dirpath + file, 'r',encoding=\"utf-8\", errors='replace') as f:\n",
    "        data=str(f.read());\n",
    "        data=data.lower()\n",
    "        data = data.replace(\"\\\"id=\", \"id=\\\"\");\n",
    "        data=data.replace(\"\\n\",\"\")\n",
    "        data=data.replace('\".*?=.*?\"', \"\",)\n",
    "        data=data.replace(\"&\",\"\");\n",
    "        xml = ET.fromstring(str(data))\n",
    "        name=None;\n",
    "        rows_list=[];\n",
    "        catchphrases=[];\n",
    "        sentences=[];\n",
    "        for child in xml:\n",
    "            if child.tag==\"catchphrases\":\n",
    "                for catchphrase in child:\n",
    "                    id=catchphrase.attrib.get(\"id\")\n",
    "                    #print(catchphrase.text)\n",
    "                    #catchphrases.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":catchphrase.text})\n",
    "                    catchphrases+=tokenizer.tokenize(catchphrase.text)\n",
    "            if child.tag==\"sentences\":\n",
    "                for sentence in child:\n",
    "                    id = sentence.attrib.get(\"id\")\n",
    "                    #sentences.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":sentence.text})\n",
    "                    sentences+=tokenizer.tokenize(sentence.text)\n",
    "    \n",
    "    sentences = [w for w in sentences if not w in stop_words_list] \n",
    "    catchphrases = [w for w in catchphrases if not w in stop_words_list]\n",
    "    \n",
    "    #sentences=[word.lower() for word in sentences if word.isalphanum()]\n",
    "    #catchphrases=[word.lower() for word in catchphrases if word.isalphanum()]\n",
    "    \n",
    "    return sentences,catchphrases\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_index_matrices(sentence_indexes):\n",
    "    length=len(sentence_indexes);\n",
    "    \n",
    "    sentence_indexes=[word_index['unk'],word_index['unk']]+sentence_indexes+[word_index['unk'],word_index['unk']];    \n",
    "    out_indices=[]\n",
    "    for i in range(2,length):\n",
    "        out_indices.append(sentence_indexes[i-2:i+3]);\n",
    "        \n",
    "    return out_indices;\n",
    "\n",
    "def prepare_label_matrices(labels):\n",
    "    length=len(labels);\n",
    "    labels=[0,0]+labels+[0,0];\n",
    "    out_labels=[];\n",
    "    \n",
    "    for i in range(2,length):\n",
    "        out_labels.append(labels[i]);\n",
    "        \n",
    "    return out_labels;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_indexes_labels(sentences,catchphrases):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    labels=[];\n",
    "    \n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "            \n",
    "    for word in sentences:\n",
    "        if word in catchphrases:\n",
    "            labels.append(1.0);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            labels.append(0.0);\n",
    "            \n",
    "    return sentence_indexes,labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5eac7f3e7a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcatchphrases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence_indexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlookup_indexes_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcatchphrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentence_index_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_index_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "sentences,catchphrases=get_statements(files[0]);\n",
    "sentence_indexes,labels=lookup_indexes_labels(sentences,catchphrases)\n",
    "print(len(sentence_indexes),len(labels))\n",
    "\n",
    "sentence_index_matrix=prepare_index_matrices(sentence_indexes);\n",
    "label_matrix=prepare_label_matrices(labels);\n",
    "\n",
    "print(len(sentence_index_matrix),len(label_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CatchPhraseExtractor(object):\n",
    "\n",
    "    def __init__(self, graph):\n",
    "        self.total_loss=0.0\n",
    "        self.build_graph(graph)\n",
    "\n",
    "    def build_graph(self, graph):\n",
    "        print(\"generating graph\");\n",
    "\n",
    "        with graph.as_default():\n",
    "            self.train_word_indices = tf.placeholder(tf.int32, shape=[None,5])\n",
    "            self.train_word_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            W = tf.Variable(word_embeddings, dtype=tf.float32) \n",
    "            phrase_tensor = tf.nn.embedding_lookup(W, self.train_word_indices)\n",
    "            \n",
    "            self.train_sentence_indices = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            #S = tf.Variable(sentence_embeddings, dtype=tf.float32)\n",
    "            #sentence_vector = tf.nn.embedding_lookup(S, self.train_sentence_indices)\n",
    "            sentence_tensor = tf.convert_to_tensor(np.array([sentence_embeddings[hash_val] for hash_val in self.train_sentence_indices])\n",
    "                                                   \n",
    "            self.train_document_vector = tf.placeholder(tf.int32, shape=[1,200])\n",
    "            document_tensor = tf.convert_to_tensor(np.array([self.train_document_vector for _ in xrange(self.train_sentence_indices.shape[0])])\n",
    "                     \n",
    "            phrase_tensor = tf.reshape(phrase_tensor, [tf.shape(phrase_tensor)[0], 1000])\n",
    "            conv_filter = tf.Variable(tf.truncated_normal((1000, 200), stddev=0.1));\n",
    "            word_tensor = tf.nn.relu(tf.matmul(sentence_vectors, conv_filter));\n",
    "            \n",
    "            stacked_tensor = tf.concat(values=[word_tensor, sentence_tensor, document_tensor], axis=1)\n",
    "                                                   \n",
    "            weights_input = tf.Variable(tf.random_normal((600, 100), stddev=0.2));\n",
    "            weights_input_2 = tf.Variable(tf.random_normal((100, 1), stddev=0.2));\n",
    "            \n",
    "            bias1 = tf.Variable(tf.random_normal((1, 100), stddev=0.2));\n",
    "            bias2 = tf.Variable(tf.random_normal((1, 1), stddev=0.2));\n",
    "            \n",
    "            self.sentence_values=self.MLP(stacked_tensor, weights_input, weights_input_2, bias1, bias2);\n",
    "            \n",
    "            prediction_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.train_labels,logits=self.sentence_values)\n",
    "            self.loss=tf.reduce_mean(total_loss)\n",
    "                               \n",
    "            #using the gradient descent optimizer\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            #clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(grads)\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    def MLP(self, stacked_tensor, weights_input, weights_input_2, bias1, bias2):\n",
    "        h1 = (tf.matmul(weights_input,stacked_tensor,transpose_a=True,transpose_b=True))\n",
    "        h1 = tf.add(h1, bias1)\n",
    "        h1 = tf.nn.tanh(h1)\n",
    "        h2 = tf.matmul(weights_input_2,h1,transpose_a=True,transpose_b=False)\n",
    "        h2 = tf.add(h2, bias2)\n",
    "        h2 = tf.nn.sigmoid(h2)\n",
    "        return tf.transpose(h2)\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self,total_loss):\n",
    "        total_loss=tf.convert_to_tensor(total_loss)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            print(variables)\n",
    "        return total_loss, tape.gradient(tf.convert_to_tensor(total_loss), variables)\n",
    "        \n",
    "                                                   \n",
    "    def train(self, sess, num_steps):\n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "        total_loss=0.0;\n",
    "        for file in files[:1]:\n",
    "            sentences,catchphrases=get_statements(file);\n",
    "            \n",
    "            sentence_indexes,labels=lookup_indexes_labels(sentences,catchphrases)\n",
    "            \n",
    "            sentence_index_matrix=prepare_index_matrices(sentence_indexes);\n",
    "            label_matrix=prepare_label_matrices(labels);\n",
    "            \n",
    "            sentence_index_matrix=np.asarray(sentence_index_matrix)\n",
    "            label_matrix=np.asarray(label_matrix)\n",
    "        \n",
    "            #sentence_indexes=np.reshape(sentence_indexes,(sentence_indexes.shape[0],1))\n",
    "            label_matrix=np.reshape(label_matrix,(label_matrix.shape[0],1))\n",
    "            \n",
    "            \n",
    "            feed_dict = {self.train_inputs: sentence_index_matrix, self.train_labels: label_matrix}\n",
    "            #loss_val = sess.run([self.loss], feed_dict=feed_dict)  \n",
    "            _,loss_val,outputs = sess.run([self.app,self.loss,self.sentence_values], feed_dict=feed_dict)\n",
    "\n",
    "            for i in range(len(outputs)):\n",
    "                if label_matrix[i]==1.0:\n",
    "                    print(outputs[i],label_matrix[i])\n",
    "            print(\"Train Finished.\",loss_val)\n",
    "        \n",
    "    def load_embeddings(self,embedding_array):\n",
    "        sess.run(self.embedding_init, feed_dict={self.embedding_placeholder: np.asarray(list(embedding_array.values()))})\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(inputs):\n",
    "    out=np.zeros((len(inputs),200));\n",
    "    for i in range(len(inputs)):\n",
    "        try:\n",
    "            c = w2v[inputs[i]]\n",
    "        except KeyError:\n",
    "            c = np.zeros((1,200));\n",
    "        out[i]=c;\n",
    "    \n",
    "    return np.transpose(out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating graph\n",
      "Tensor(\"Max:0\", shape=(1, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "model = CatchPhraseExtractor(graph, embedding_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initailized\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "[0.466128] [1.]\n",
      "Train Finished. 81.37931\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "  \n",
    "    model.load_embeddings(embedding_array)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    for i in range(1):\n",
    "        model.train(sess, 10)   \n",
    "    #optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
