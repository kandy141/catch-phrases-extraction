{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile,join\n",
    "import xml.etree.cElementTree as ET\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "#import doc2vec_impl\n",
    "import gensim\n",
    "\n",
    "import FileProcess as fp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print (fp.window_size)\n",
    "batch_size=10\n",
    "embedding_filename = \"glove_6B_200d.txt\"\n",
    "training_epochs=1\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"corpus/fulltext/\"\n",
    "files = [f for f in os.listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.7762175  -0.45936382 -0.618449   -0.8609642   0.12776479  0.7008039\n",
      " -0.18298188 -0.3065114  -1.0207447   0.44748068 -0.5251434  -0.6478685\n",
      "  0.07363328 -0.9913516  -0.08037782  0.3569275   0.04830581 -0.3157292\n",
      " -0.33566946 -1.8007975  -0.6615384  -0.45662943  0.35856095 -1.6224141\n",
      "  1.7292947   0.01610192  0.15269798 -0.2633624  -0.42103472  0.4310425\n",
      " -0.8186927  -1.817319   -1.9041965  -0.8068781  -1.777631   -1.8509216\n",
      " -0.4281155   0.4977019   0.61602515  0.76634425  1.3259608  -0.31260923\n",
      " -1.3816838  -0.37780055 -1.1810374  -1.6388259   1.0663178   0.06534389\n",
      " -0.7727587  -0.11205402 -1.9782453   0.08237717  1.9480875  -1.8146058\n",
      " -1.3543537   1.3657124  -0.25756338  0.54493374 -0.6701863   0.32007536\n",
      "  0.05760738  1.294205   -0.32358631 -0.9725704  -1.0321938  -0.39936873\n",
      "  0.15072757 -0.7044433  -1.2028613  -0.57092947 -1.0215961   2.1046119\n",
      " -0.02798054 -0.17244416  0.8805654   0.5527702   1.502657   -2.0060904\n",
      "  0.67571783  1.7193362   1.0933233  -1.2774023   0.8924701   0.8151471\n",
      "  0.06117444 -0.8534738   2.235452    1.6948946  -1.0923411  -0.12216213\n",
      " -0.6072777  -0.7575913  -1.4340287  -1.2235746   0.306698   -1.6047995\n",
      "  0.6615473   0.7902758   0.33293596  0.19390744  1.5244843  -0.39736405\n",
      " -0.37221682 -1.5747931   1.4609582   0.0366693  -0.5378543   3.19066\n",
      "  3.3277166  -1.0259914   0.99693525 -0.7980915   0.3343151  -0.80774\n",
      " -0.50230837  1.9319516  -0.91713965  0.7356805   0.3080322  -0.997319\n",
      " -0.7065433   0.914548    0.2810213   0.59405327  1.6729107   0.6439485\n",
      " -2.1773677   0.47063777 -0.8087191  -0.9846444   0.38202858  0.14417563\n",
      " -0.74225235 -0.13711248  1.9400598  -0.37968934  0.6087961   0.8637753\n",
      " -1.0023636  -0.6397425   0.1575091   0.6904777  -0.30774748  0.24007772\n",
      " -1.9852216  -0.79431224 -0.27392206  1.3890268  -0.25229788 -0.92108697\n",
      " -1.6003761  -1.4999192   1.0460986   0.4968402  -1.0209701  -1.0875626\n",
      " -1.1576102  -0.6412773  -0.07398531 -1.4260368   0.2522694  -0.6771478\n",
      "  0.3652671   0.7851982   0.46740776 -0.50511736  0.27771106  0.37511614\n",
      "  1.1110052   1.5658243   1.0335125   0.21563636  0.38487524  0.5318744\n",
      "  0.49647796 -1.1856257   0.5468444   0.22220357 -1.2082943  -1.0197014\n",
      " -0.45458066  1.0694165   2.0186963  -0.84617615 -0.05715404  0.40114826\n",
      "  0.16749382  0.4192865   0.39392647 -0.4010377  -1.855592   -0.5995523\n",
      " -0.14247629  0.13453192  0.05007014  0.91260254  0.34311357 -1.1139953\n",
      " -1.2834044  -1.525633  ]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list=set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "##########loading dev2vec=============\n",
    "#doc2vec_impl.main();\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('doc2vec_models/doc2vec.model')\n",
    "#start testing\n",
    "#printing the vector of document at index 1 in docLabels\n",
    "docvec = d2v_model.docvecs[files[0]]\n",
    "print(docvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    dvocab, w2v,word_index  = read_data(filename);\n",
    "    return w2v,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Words in DataSet:', 400000)\n",
      "Loaded embeddings\n",
      "reading data completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_array,word_index = load_embeddings(embedding_filename)\n",
    "print(\"Loaded embeddings\")\n",
    "\n",
    "# words_data=pd.read_csv(\"words_data.csv\");\n",
    "# catchphrases_data=pd.read_csv(\"catch_data.csv\")\n",
    "print(\"reading data completed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sen2vec embeddings\n",
    "def sen2vec(sentences):\n",
    "    embed = hub.Module(module_url)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        sentenceEmbeddings = session.run(embed(sentences))    \n",
    "        return sentenceEmbeddings\n",
    "\n",
    "# sentence_embeddings = sen2Vec(['UWA was ordered to pay the costs of Dr Gra','Sirtex succeeded in its cross-claim against Dr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index['unk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(file):\n",
    "    with open(dirpath + file, 'r',encoding=\"utf-8\", errors='replace') as f:\n",
    "        data=str(f.read());\n",
    "        data=data.lower()\n",
    "        data = data.replace(\"\\\"id=\", \"id=\\\"\");\n",
    "        data=data.replace(\"\\n\",\"\")\n",
    "        data=data.replace('\".*?=.*?\"', \"\",)\n",
    "        data=data.replace(\"&\",\"\");\n",
    "        xml = ET.fromstring(str(data))\n",
    "        name=None;\n",
    "        rows_list=[];\n",
    "        catchphrases=[];\n",
    "        sentences=[];\n",
    "        for child in xml:\n",
    "            if child.tag==\"catchphrases\":\n",
    "                for catchphrase in child:\n",
    "                    id=catchphrase.attrib.get(\"id\")\n",
    "                    #print(catchphrase.text)\n",
    "                    #catchphrases.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":catchphrase.text})\n",
    "                    catchphrases+=tokenizer.tokenize(catchphrase.text)\n",
    "            if child.tag==\"sentences\":\n",
    "                for sentence in child:\n",
    "                    id = sentence.attrib.get(\"id\")\n",
    "                    #sentences.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":sentence.text})\n",
    "                    sentences+=tokenizer.tokenize(sentence.text)\n",
    "    \n",
    "    sentences = [w for w in sentences if not w in stop_words_list] \n",
    "    catchphrases = [w for w in catchphrases if not w in stop_words_list]\n",
    "    \n",
    "    #sentences=[word.lower() for word in sentences if word.isalphanum()]\n",
    "    #catchphrases=[word.lower() for word in catchphrases if word.isalphanum()]\n",
    "    \n",
    "    return sentences,catchphrases\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_index_matrices(sentence_indexes):\n",
    "    length=len(sentence_indexes);\n",
    "    \n",
    "    sentence_indexes=[word_index['unk'],word_index['unk']]+sentence_indexes+[word_index['unk'],word_index['unk']];    \n",
    "    out_indices=[]\n",
    "    for i in range(2,length):\n",
    "        out_indices.append(sentence_indexes[i-2:i+3]);\n",
    "        \n",
    "    return out_indices;\n",
    "\n",
    "def prepare_label_matrices(labels):\n",
    "    length=len(labels);\n",
    "    labels=[0,0]+labels+[0,0];\n",
    "    out_labels=[];\n",
    "    \n",
    "    for i in range(2,length):\n",
    "        out_labels.append(labels[i]);\n",
    "        \n",
    "    return out_labels;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_indexes_labels(sentences,catchphrases):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    labels=[];\n",
    "    \n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "            \n",
    "    for word in sentences:\n",
    "        if word in catchphrases:\n",
    "            labels.append(1.0);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            labels.append(0.0);\n",
    "            \n",
    "    return sentence_indexes,labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_sentences(file_name):\n",
    "    temp_data=full_data[(full_data.file_id==file_name) & full_data['Id'].str.startswith(('s'))]\n",
    "    return list(temp_data.text.values)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CatchPhraseExtractor(object):\n",
    "\n",
    "    def __init__(self, graph):\n",
    "        self.total_loss=0.0\n",
    "        self.build_graph(graph)\n",
    "\n",
    "    def build_graph(self, graph):\n",
    "        print(\"generating graph\");\n",
    "\n",
    "        with graph.as_default():\n",
    "            \n",
    "            #feed_dict = {self.doc_embedding: doc_embedding, self.phrases_indices: phrases_indices, self.phrase_labels: phrases_label_matrix,\n",
    "            #self.sentence_indices:sentence_ids_to_indices, self.sentence_embeddings: sentence_embeddings}            \n",
    "            \n",
    "            \n",
    "            self.phrases_indices = tf.placeholder(tf.int32, shape=[None,5])\n",
    "            self.phrase_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            W = tf.Variable(embedding_array, dtype=tf.float32) \n",
    "            phrase_tensor = tf.nn.embedding_lookup(W, self.phrases_indices)\n",
    "            \n",
    "            self.sentence_ids_to_indices = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            self.sentence_embeddings = tf.placeholder(tf.int32, shape=[None,200])\n",
    "            sentence_tensor = tf.nn.embedding_lookup(self.sentence_embeddings, self.sentence_ids_to_indices)\n",
    "                                                   \n",
    "            self.doc_embedding = tf.placeholder(tf.int32, shape=[1,200])\n",
    "            document_tensor = tf.convert_to_tensor(np.array([self.doc_embedding for _ in xrange(self.train_sentence_indices.shape[0])]))\n",
    "                     \n",
    "            phrase_tensor = tf.reshape(phrase_tensor, [tf.shape(phrase_tensor)[0], 1000])\n",
    "            conv_filter = tf.Variable(tf.truncated_normal((1000, 200), stddev=0.1));\n",
    "            word_tensor = tf.nn.relu(tf.matmul(sentence_vectors, conv_filter));\n",
    "            \n",
    "            stacked_tensor = tf.concat(values=[word_tensor, sentence_tensor, document_tensor], axis=1)\n",
    "                                                   \n",
    "            weights_input = tf.Variable(tf.random_normal((600, 100), stddev=0.2));\n",
    "            weights_input_2 = tf.Variable(tf.random_normal((100, 1), stddev=0.2));\n",
    "            \n",
    "            bias1 = tf.Variable(tf.random_normal((1, 100), stddev=0.2));\n",
    "            bias2 = tf.Variable(tf.random_normal((1, 1), stddev=0.2));\n",
    "            \n",
    "            self.sentence_values=self.MLP(stacked_tensor, weights_input, weights_input_2, bias1, bias2);\n",
    "            \n",
    "            prediction_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.train_labels,logits=self.sentence_values)\n",
    "            self.loss=tf.reduce_mean(total_loss)\n",
    "                               \n",
    "            #using the gradient descent optimizer\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            #clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(grads)\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    def MLP(self, stacked_tensor, weights_input, weights_input_2, bias1, bias2):\n",
    "        h1 = (tf.matmul(weights_input,stacked_tensor,transpose_a=True,transpose_b=True))\n",
    "        h1 = tf.add(h1, bias1)\n",
    "        h1 = tf.nn.tanh(h1)\n",
    "        h2 = tf.matmul(weights_input_2,h1,transpose_a=True,transpose_b=False)\n",
    "        h2 = tf.add(h2, bias2)\n",
    "        h2 = tf.nn.sigmoid(h2)\n",
    "        return tf.transpose(h2)\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self,total_loss):\n",
    "        total_loss=tf.convert_to_tensor(total_loss)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            print(variables)\n",
    "        return total_loss, tape.gradient(tf.convert_to_tensor(total_loss), variables)\n",
    "        \n",
    "                                                   \n",
    "    def train(self, sess, num_steps):\n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "        total_loss=0.0;\n",
    "        for file in files[:1]:            \n",
    "            file_data_frame, catch_words = fp.get_dataframe(file)\n",
    "            sentences_list = get_file_sentences(file) #indexes/order preserved\n",
    "            sentence_embeddings = sen2vec(sentences_list)\n",
    "            sentence_ids_to_indices = np.array([int(x[1:]) for x in list(file_data_frame['Id'].values)])\n",
    "            phrases_indices = np.array(list(file_dataframe['words'].values))\n",
    "            phrases_label_matrix = np.array(list(file_dataframe['is_catchword'].values))\n",
    "            doc_embedding = d2v_model.docvecs[file]\n",
    "            feed_dict = {self.doc_embedding: doc_embedding, self.phrases_indices: phrases_indices, self.phrase_labels: phrases_label_matrix, self.sentence_indices:sentence_ids_to_indices, self.sentence_embeddings: sentence_embeddings}            \n",
    "            #loss_val = sess.run([self.loss], feed_dict=feed_dict)  \n",
    "            _,loss_val,outputs = sess.run([self.app,self.loss,self.sentence_values], feed_dict=feed_dict)\n",
    "\n",
    "            print(\"Train Finished: loss_val=\", loss_val)\n",
    "            for i in range(len(outputs)):\n",
    "                print(outputs[i])\n",
    "            \n",
    "        \n",
    "    def load_embeddings(self,embedding_array):\n",
    "        sess.run(self.embedding_init, feed_dict={self.embedding_placeholder: np.asarray(list(embedding_array.values()))})\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(inputs):\n",
    "    out=np.zeros((len(inputs),200));\n",
    "    for i in range(len(inputs)):\n",
    "        try:\n",
    "            c = w2v[inputs[i]]\n",
    "        except KeyError:\n",
    "            c = np.zeros((1,200));\n",
    "        out[i]=c;\n",
    "    \n",
    "    return np.transpose(out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating graph\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "model = CatchPhraseExtractor(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "  \n",
    "    #model.load_embeddings(embedding_array)    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    #global_step = tf.train.get_or_create_global_step()\n",
    "    for i in range(1):\n",
    "        model.train(sess, 10)\n",
    "    #optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
