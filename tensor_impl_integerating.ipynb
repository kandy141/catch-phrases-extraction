{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Words in DataSet:', 400000)\n",
      "('shape of data:', (866842, 5))\n",
      "Started\n",
      "('time-taken:', 9.5367431640625e-07)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile,join\n",
    "import xml.etree.cElementTree as ET\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "#import doc2vec_impl\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import FileProcess as fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print (fp.window_size)\n",
    "batch_size=10\n",
    "vocabulary_size=400000\n",
    "embedding_size=200\n",
    "embedding_filename = \"glove_6B_200d.txt\"\n",
    "training_epochs=1\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' background  1 the applicant is a citizen of the peoples republic of china (prc).', 'he arrived in australia on 25 december 2002.', 'on 14 january 2003 the applicant applied for a protection (class xa) visa.', 'on 18 june 2003 a delegate of the minister for immigration and multicultural affairs (as he was then) refused to grant the applicant a protection (class xa) visa (\"delegate\\'s decision\").', \"on 7 july 2003 the applicant applied to the refugee review tribunal for review of the delegate's decision.\", 'on 5 may 2004 the applicant gave oral evidence at a tribunal hearing.', \"on 3 march 2005 the tribunal made a decision affirming the delegate's decision.\", 'on 23 march 2006 federal magistrate lloyd-jones made orders by consent remitting the matter to the tribunal.', 'on 24 may 2006 the tribunal, as reconstituted, sent a letter, by facsimile, inviting the applicant to attend a hearing.', 'on 26 may 2006 the tribunal received a \"response to hearing invitation\" form indicating that the applicant would attend the hearing.', 'on 8 june 2006 a differently constituted tribunal held its hearing which the applicant attended and gave oral evidence.', 'the account of the evidence given by the applicant at the 5 may 2004 hearing contained in the decision of the previous member was, without objection by the applicant, treated as accurate.', \"on 26 july 2006 the tribunal, as reconstituted, made its decision affirming the delegate's decision to refuse to grant the applicant a protection (class xa) visa.\", 'on 15 august 2006 the tribunal handed down its decision.', 'on 29 august 2006 the applicant filed an application for judicial review in the federal magistrates court.', 'on 27 october 2006 the matter was listed for a show cause hearing: rule 44.12(1) of the federal magistrates court rules .', 'the federal magistrate dismissed the applicant\\'s application as \"it fails to disclose an arguable case of jurisdictional error\".', 'this was a dismissal under rule 44.12(1)(a) and by virtue of rule 44.12(2) was interlocutory.', \"on 13 november 2006 the applicant filed an application for leave to appeal from federal magistrate driver's judgment.\", 'proceedings in the tribunal  2 before the tribunal the appellant claimed to have a well founded fear of persecution because of his practise of falun gong in the prc.', 'the appellant claimed he was elected team leader of the falun gong tutorial centre and promoted falun gong in his spare time as well as participating in a \"sit-in\" in beijing.', 'the appellant claimed that police attended his work place asking him to provide a statement he would not practise falun gong.', 'the appellant asserted that he refused and was dismissed from his employment.', 'the appellant claims he continued to practise falun gong in secret and that police attended the house of a fellow falun gong practitioner while the appellant was there practising falun gong and arrested the appellant and his friend: both being subsequently sentenced to a labour camp for one year and two years respectively.', 'after his release the appellant claims that his new employer was requested to \"watch the appellant\" to see if he continued to practise falun gong.', 'the appellant claims he considered departing the prc.', 'the appellant claimed that he feared that he would be gaoled for his falun gong practice if he returned to the prc.', 'the appellant claimed that in australia he had practised falun gong and written an article about how he joined falun gong for a newspaper called \"the truth\", a falun gong journal.', '3 in support of his claims the appellant included an article said to have been written by him in chinese and published in \"truth\", a falun gong journal in australia, a letter from his local police station outlining the appellant\\'s practise of falun gong, an article from a falun gong website outlining the arrest of the appellant\\'s friend, a bank receipt showing the appellant had sent money to the wife of his friend who had been sentenced to two years imprisonment and photographs of him practising falun gong and distributing falun gong material in australia.', 'the decision of the tribunal  4 in its very lengthy and detailed reasons the tribunal found the appellant to be a thoroughly unconvincing witness.', 'the tribunal was left with a very strong impression that the appellant was not speaking from actual personal experience.', 'there were many inconsistencies between the written claims before the tribunal and his oral evidence which were unable to be explained at hearing.', 'the tribunal found that the evidence of the appellant with regard to the protest in 1999 was inconsistent with independent country information.', \"the tribunal concluded that the appellant was not a credible witness and accordingly did not give any weight to the appellant's supposed corroborative evidence.\", 'moreover, the tribunal was satisfied that the appellant\\'s falun gong activities in australia was conduct engaged in for the sole purpose of strengthening his refugee claims and disregarded it for that reason: see s 91r(3) of the migration act 1958 (cth) (\"the act \").', 'grounds before the federal magistrate  5 on 29 august 2006 the appellant applied for judicial review of the decision of the tribunal and before the federal magistrate the appellant claimed that: a breach of the rules of natural justice occurred in connection with the making of the decision; procedures which were required to be observed were not observed; the decision of the tribunal was an improper exercise of power; natural justice was denied as the context in which the appellant would face serious harm was not considered and the respondents only considered evidence not in favour of the appellant.', \"the decision of the federal magistrate  6 prior to the 'show cause' hearing the applicant had been given the opportunity to file and serve an amended application and additional evidence, including a transcript of the tribunal hearing by 17 october 2006.\", 'he did not take up that opportunity.', '7 the federal magistrate found that the show cause application failed to disclose an arguable case of jurisdictional error and nor was any jurisdictional error apparent from the available material before him.', \"it is convenient to set out the federal magistrate's reasons relevant to these grounds.\", '   5.', 'i invited the applicant to explain those grounds in his oral submissions.', 'he was not able to do so apart from making claims of interpretation problems at the hearing.', 'there are several difficulties with that assertion.', 'the first is that there is no mention of interpretation difficulties in the show cause application or its supporting affidavit.', 'the second is that the applicant has not taken up the opportunity i gave him to produce a transcript which might, hypothetically, have supported the assertion.', 'the third is that the record of the tribunal decision does not provide any support to the allegation of interpretation difficulties.', '  6.', 'the record of the tribunal decision does establish clearly that the applicant failed before the tribunal because he was not believed.', \"after listening to the applicant's evidence at the hearing, the presiding member was left in a state of complete disbelief.\", 'it is apparent that the presiding member considered that the well was poisoned and he would not continue to drink from it.', \"the presiding member thus gave no weight to a letter which might have corroborated some of the applicant's claims, see court book page 126.\", 'having regard to the force of the adverse credibility finding made by the tribunal, i see no error in that approach.', '  7.', 'the show cause application asserts a breach of the rules of natural justice.', \"to the extent that the particulars throw any light on that assertion, they appear to relate to a failure to consider the applicant's claims in their proper context.\", 'the applicant was not able to explain this assertion further.', \"it appears to me from the record of the tribunal decision that the presiding member both understood the applicant's claims and considered them.\", '  8.', 'secondly, the show cause application asserts a failure to observe procedures required by law.', 'it appears to me from the record of the tribunal decision that the tribunal met its statutory obligations under the migration act 1958 (cth) (\"the migration act \").', 'in my view, this ground is not arguable.', '  9.', 'thirdly, the show cause application asserts that the making of the decision was an improper exercise of the power conferred by the migration act .', 'the asserted particular appears to be an allegation that the tribunal limited itself to a consideration of unfavourable evidence.', 'in one sense, that is true.', 'the decision turns entirely on the evidence given by the applicant at the hearing conducted by the tribunal.', 'the presiding member found that evidence to be wholly unfavourable.', 'where a decision-maker is left in a state of complete disbelief after hearing from an applicant, that is in my view a sufficient basis to dispose of an application.', '  10.', 'i conclude that the show cause application fails to disclose an arguable case of jurisdictional error.', 'neither is any arguable claim of jurisdictional error apparent to me from the available material.', 'i will therefore order that the application be dismissed pursuant to rule 44.12(1)(a) of the federal magistrates court rules 2001 (cth).', ' 8 the federal magistrate ordered that the application be dismissed pursuant to rule 44.12(1)(a) of the federal magistrates court rules 2001 (cth).', 'notice of appeal  9 on 13 november 2006 the appellant filed a notice of appeal.', 'it is to be treated, in the circumstances as a draft notice of appeal.', 'the first respondent was content for the court to follow this course.', 'the notice of appeal raises the following grounds which are, in effect, a repetition of the grounds before the federal magistrate.', '  1.', 'the tribunal did not comply with s424a amp; s 441a of the migration act 1958 .', '      2.', 'the tribunal breached the rules of natural justice in connection with the making of the decision.', 'the respondents denied the applicant natural justice by not considering the context in which the applicant will face persecution and serious harm for being a falun gong practitioner in china.', '      3.', 'the making of the decision was an improper exercise of power conferred by the enactment in pursuance of which it was purported to be made.', '      4.', 'the respondents have no considered the amount of evidences which are in favour of the applicant.', \"the respondents regards all the evidences as produced for the sole purpose of strengthening the applicant's refugee claim.\", 'they have only considered the evidence which is not in favour of the applicant.', '    (transcribed from the original without alteration) 10 i have considered the reasons of both the tribunal and the federal magistrate.', 'none of these grounds, in my opinion, discloses any jurisdictional error on the part of the learned federal magistrate.', 'it was well open to his honour to exercise his discretion pursuant to r 44.12(1)(a) and to dismiss the application without it going to a final hearing.', 'my reasons, which follow, essentially reflect the submissions put by the first respondent.', '11 the test for whether leave to appeal is granted or refused is well established: deacute;cor corp v dart industries inc (1991) 3 fcr 397 at [9].', \"12 the applicant's grounds contained in his notice of appeal do not identify any error in the federal magistrate's decision.\", 'rather, the grounds are directed to the decision of the tribunal.', '13 the applicant alleges that the tribunal failed to comply with sections 424a and 441a of the act .', 'this ground is unparticularised.', 'the tribunal as first constituted wrote to the applicant inviting his comment on two occasions.', \"the tribunal was not required to write to the applicant, as reconstituted, in respect of any information upon which it relied as its findings relied on the applicant's evidence given to the tribunal, both as originally constituted and as reconstituted and the documents provided by the applicant to the tribunal.\", 'accordingly, there is no merit to this ground.', '14 at the hearing before me the applicant emphasised that he had not had a fair hearing before the tribunal because he was denied the opportunity of putting on important additional evidence.', '15 when i asked him about the orders which the federal magistrate had made on 12 september 2006 for the filing by 17 october 2006 of additional material including further affidavit evidence including a transcript of the tribunal hearing he said that he knew that he had to provide further evidence by 17 october but thought that the pro bono legal adviser would attend to this.', '16 the applicant was present before the federal magistrate, assisted, he told me, by an interpreter.', 'no such explanation was given to the federal magistrate.', 'his honour did state however that the applicant did not act on the opportunities provided by the orders of 12 september 2006.', \"17 the applicant's second ground relates to a purported breach of the rules of natural justice by the tribunal in not considering the context in which the applicant would have faced persecution and serious harm by reason of being a falun gong practitioner in china.\", 'section 422b of the act excludes the common law rules of natural justice.', \"insofar as the applicant alleges that the tribunal failed to consider the applicant's refugee claims, it is clear that this is not the case.\", \"the tribunal considered and rejected the applicant's claims that he feared persecution in china by reason of his practice of falun gong.\", '18 the third ground alleges that the \"making of the decision was an improper exercise of the power conferred by the enactment in pursuance of which it was purported to be made\".', 'this is a generic, unparticularised ground.', 'it is without substance.', '19 the fourth ground alleges that the tribunal failed to consider evidence in favour of the applicant and that it only considered evidence which was not in favour of the applicant.', 'to the extent that this is an allegation of bias the applicant has not met the requirement that such an allegation be firmly and distinctly made and clearly proved: minister for immigration amp; multicultural affairs v jia le geng [2001] hca 17 ; (2001) 205 clr 507 at [530] .', '20 to the extent that the tribunal, upon considering the evidence as a whole (which it expressly stated it had done) gave no weight to evidence on its face favourable to the applicant it did so because it found the applicant not to be a credible witness.', 'no error and in particular no jurisdictional error arises by reason of that approach which was clearly open to it.', \"21 accordingly i am of the opinion that the federal magistrate's decision is not attended by sufficient doubt to warrant it being reconsidered by the federal court.\", 'whilst the applicant disagrees with the decision below he has been unable to point to any relevant error.', 'it is evident that no substantial injustice would result from my refusal of leave.', '22 accordingly, the application for leave to appeal ought be dismissed with costs.', 'i certify that the preceding twenty-two (22) numbered paragraphs are a true copy of the reasons for judgment herein of the honourable justice gilmour.', 'associate:dated: 26 february 2007 the appellant appeared in person.', 'solicitor for the respondent: z. chami   date of hearing: 26 february 2007   date of judgment: 26 february 2007    austlii: copyright policy | disclaimers | privacy policy | feedback  url: http://www.austlii.edu.au/au/cases/cth/fca/2007/209.html   ']\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"corpus/fulltext/\"\n",
    "files = [f for f in os.listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "#print(fp.get_sentences(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.7762175  -0.45936382 -0.618449   -0.8609642   0.12776479  0.7008039\n",
      " -0.18298188 -0.3065114  -1.0207447   0.44748068 -0.5251434  -0.6478685\n",
      "  0.07363328 -0.9913516  -0.08037782  0.3569275   0.04830581 -0.3157292\n",
      " -0.33566946 -1.8007975  -0.6615384  -0.45662943  0.35856095 -1.6224141\n",
      "  1.7292947   0.01610192  0.15269798 -0.2633624  -0.42103472  0.4310425\n",
      " -0.8186927  -1.817319   -1.9041965  -0.8068781  -1.777631   -1.8509216\n",
      " -0.4281155   0.4977019   0.61602515  0.76634425  1.3259608  -0.31260923\n",
      " -1.3816838  -0.37780055 -1.1810374  -1.6388259   1.0663178   0.06534389\n",
      " -0.7727587  -0.11205402 -1.9782453   0.08237717  1.9480875  -1.8146058\n",
      " -1.3543537   1.3657124  -0.25756338  0.54493374 -0.6701863   0.32007536\n",
      "  0.05760738  1.294205   -0.32358631 -0.9725704  -1.0321938  -0.39936873\n",
      "  0.15072757 -0.7044433  -1.2028613  -0.57092947 -1.0215961   2.1046119\n",
      " -0.02798054 -0.17244416  0.8805654   0.5527702   1.502657   -2.0060904\n",
      "  0.67571783  1.7193362   1.0933233  -1.2774023   0.8924701   0.8151471\n",
      "  0.06117444 -0.8534738   2.235452    1.6948946  -1.0923411  -0.12216213\n",
      " -0.6072777  -0.7575913  -1.4340287  -1.2235746   0.306698   -1.6047995\n",
      "  0.6615473   0.7902758   0.33293596  0.19390744  1.5244843  -0.39736405\n",
      " -0.37221682 -1.5747931   1.4609582   0.0366693  -0.5378543   3.19066\n",
      "  3.3277166  -1.0259914   0.99693525 -0.7980915   0.3343151  -0.80774\n",
      " -0.50230837  1.9319516  -0.91713965  0.7356805   0.3080322  -0.997319\n",
      " -0.7065433   0.914548    0.2810213   0.59405327  1.6729107   0.6439485\n",
      " -2.1773677   0.47063777 -0.8087191  -0.9846444   0.38202858  0.14417563\n",
      " -0.74225235 -0.13711248  1.9400598  -0.37968934  0.6087961   0.8637753\n",
      " -1.0023636  -0.6397425   0.1575091   0.6904777  -0.30774748  0.24007772\n",
      " -1.9852216  -0.79431224 -0.27392206  1.3890268  -0.25229788 -0.92108697\n",
      " -1.6003761  -1.4999192   1.0460986   0.4968402  -1.0209701  -1.0875626\n",
      " -1.1576102  -0.6412773  -0.07398531 -1.4260368   0.2522694  -0.6771478\n",
      "  0.3652671   0.7851982   0.46740776 -0.50511736  0.27771106  0.37511614\n",
      "  1.1110052   1.5658243   1.0335125   0.21563636  0.38487524  0.5318744\n",
      "  0.49647796 -1.1856257   0.5468444   0.22220357 -1.2082943  -1.0197014\n",
      " -0.45458066  1.0694165   2.0186963  -0.84617615 -0.05715404  0.40114826\n",
      "  0.16749382  0.4192865   0.39392647 -0.4010377  -1.855592   -0.5995523\n",
      " -0.14247629  0.13453192  0.05007014  0.91260254  0.34311357 -1.1139953\n",
      " -1.2834044  -1.525633  ]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list=set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "##########loading dev2vec=============\n",
    "#doc2vec_impl.main();\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('doc2vec_models/doc2vec.model')\n",
    "#start testing\n",
    "#printing the vector of document at index 1 in docLabels\n",
    "docvec = d2v_model.docvecs[files[0]]\n",
    "#print(docvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    dvocab, w2v,word_index  = read_data(filename);\n",
    "    return w2v,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Words in DataSet:', 400000)\n",
      "Loaded embeddings\n",
      "reading data completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_array,word_index = load_embeddings(embedding_filename)\n",
    "print(\"Loaded embeddings\")\n",
    "\n",
    "# words_data=pd.read_csv(\"words_data.csv\");\n",
    "# catchphrases_data=pd.read_csv(\"catch_data.csv\")\n",
    "print(\"reading data completed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO avoid downloading tf hub modules everytime\n",
    "##  export TFHUB_CACHE_DIR=/usr/local/bin\n",
    "\n",
    "#sen2vec embeddings\n",
    "def sen2vec1(sentences):\n",
    "    embed = hub.Module(module_url)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        sentenceEmbeddings = session.run(embed(sentences))    \n",
    "        return sentenceEmbeddings\n",
    "\n",
    "    \n",
    "def sent2vector(sent):\n",
    "    words = word_tokenize(sent.lower())\n",
    "    emb=[]\n",
    "    for w in words:\n",
    "        if w in embedding_array.keys():\n",
    "            emb.append(embedding_array.get(w));\n",
    "        else:\n",
    "            emb.append(embedding_array.get('unk'));\n",
    "    return np.mean(np.array(emb),axis=0)\n",
    "\n",
    "def get_sent_embeddings(sent_list):\n",
    "    out_vec=np.zeros((len(sent_list)-1,200))\n",
    "    for i  in range(len(sent_list)-1):\n",
    "        out_vec[i]=sent2vector(sent_list[i]);\n",
    "    return out_vec\n",
    "    \n",
    "# sentence_embeddings = sen2Vec(['UWA was ordered to pay the costs of Dr Gra','Sirtex succeeded in its cross-claim against Dr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(file):\n",
    "    #with open(dirpath + file, 'r',encoding=\"utf-8\", errors='replace') as f:\n",
    "    with open(dirpath + file, 'r') as f:\n",
    "        data=str(f.read());\n",
    "        data=data.lower()\n",
    "        data = data.replace(\"\\\"id=\", \"id=\\\"\");\n",
    "        data=data.replace(\"\\n\",\"\")\n",
    "        data=data.replace('\".*?=.*?\"', \"\",)\n",
    "        data=data.replace(\"&\",\"\");\n",
    "        xml = ET.fromstring(str(data))\n",
    "        name=None;\n",
    "        rows_list=[];\n",
    "        catchphrases=[];\n",
    "        sentences=[];\n",
    "        for child in xml:\n",
    "            if child.tag==\"catchphrases\":\n",
    "                for catchphrase in child:\n",
    "                    id=catchphrase.attrib.get(\"id\")\n",
    "                    #print(catchphrase.text)\n",
    "                    catchphrases.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":catchphrase.text})\n",
    "                    #catchphrases+=tokenizer.tokenize(catchphrase.text)\n",
    "            if child.tag==\"sentences\":\n",
    "                for sentence in child:\n",
    "                    id = sentence.attrib.get(\"id\")\n",
    "                    sentences.append({\"file_id\":file,\"Name\":name,\"Id\":id,\"text\":sentence.text})\n",
    "                    #sentences+=tokenizer.tokenize(sentence.text)\n",
    "    \n",
    "    \n",
    "    #sentences = [w for w in sentences if not w in stop_words_list] \n",
    "    #catchphrases = [w for w in catchphrases if not w in stop_words_list]\n",
    "    \n",
    "    \n",
    "    #sentences=[word.lower() for word in sentences if word.isalphanum()]\n",
    "    #catchphrases=[word.lower() for word in catchphrases if word.isalphanum()]\n",
    "    \n",
    "    return sentences,catchphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(        Id     file_id  is_catchword                                words\n",
      "0       s0  07_209.xml             0            [2093, 176, 0, 19067, 14]\n",
      "1       s0  07_209.xml             1               [176, 0, 19067, 14, 7]\n",
      "2       s0  07_209.xml             0              [0, 19067, 14, 7, 3941]\n",
      "3       s0  07_209.xml             0              [19067, 14, 7, 3941, 3]\n",
      "4       s0  07_209.xml             0                  [14, 7, 3941, 3, 0]\n",
      "5       s0  07_209.xml             0                [7, 3941, 3, 0, 5527]\n",
      "6       s0  07_209.xml             0              [3941, 3, 0, 5527, 874]\n",
      "7       s0  07_209.xml             0                 [3, 0, 5527, 874, 3]\n",
      "8       s0  07_209.xml             0               [0, 5527, 874, 3, 132]\n",
      "9       s0  07_209.xml             0          [5527, 874, 3, 132, 201534]\n",
      "10      s1  07_209.xml             0               [18, 1149, 6, 603, 13]\n",
      "11      s1  07_209.xml             0              [1149, 6, 603, 13, 579]\n",
      "12      s1  07_209.xml             0               [6, 603, 13, 579, 470]\n",
      "13      s1  07_209.xml             0          [603, 13, 579, 470, 201534]\n",
      "14      s2  07_209.xml             0               [13, 657, 450, 782, 0]\n",
      "15      s2  07_209.xml             0            [657, 450, 782, 0, 19067]\n",
      "16      s2  07_209.xml             0           [450, 782, 0, 19067, 2910]\n",
      "17      s2  07_209.xml             1            [782, 0, 19067, 2910, 10]\n",
      "18      s2  07_209.xml             0              [0, 19067, 2910, 10, 7]\n",
      "19      s2  07_209.xml             0           [19067, 2910, 10, 7, 1464]\n",
      "20      s2  07_209.xml             0          [2910, 10, 7, 1464, 201534]\n",
      "21      s2  07_209.xml             0        [10, 7, 1464, 201534, 201534]\n",
      "22      s2  07_209.xml             0    [7, 1464, 201534, 201534, 201534]\n",
      "23      s3  07_209.xml             0               [13, 519, 344, 782, 7]\n",
      "24      s3  07_209.xml             0             [519, 344, 782, 7, 8179]\n",
      "25      s3  07_209.xml             0               [344, 782, 7, 8179, 3]\n",
      "26      s3  07_209.xml             0                 [782, 7, 8179, 3, 0]\n",
      "27      s3  07_209.xml             0                 [7, 8179, 3, 0, 141]\n",
      "28      s3  07_209.xml             0                [8179, 3, 0, 141, 10]\n",
      "29      s3  07_209.xml             0                [3, 0, 141, 10, 2252]\n",
      "...    ...         ...           ...                                  ...\n",
      "2028  s120  07_209.xml             0             [10, 891, 4, 1574, 6915]\n",
      "2029  s120  07_209.xml             1             [891, 4, 1574, 6915, 30]\n",
      "2030  s120  07_209.xml             0            [4, 1574, 6915, 30, 2869]\n",
      "2031  s120  07_209.xml             0           [1574, 6915, 30, 2869, 17]\n",
      "2032  s120  07_209.xml             0         [6915, 30, 2869, 17, 201534]\n",
      "2033  s121  07_209.xml             0            [41, 18428, 12, 0, 10998]\n",
      "2034  s121  07_209.xml             0         [18428, 12, 0, 10998, 32335]\n",
      "2035  s121  07_209.xml             0        [12, 0, 10998, 32335, 201534]\n",
      "2036  s121  07_209.xml             0      [0, 10998, 32335, 201534, 9730]\n",
      "2037  s121  07_209.xml             0  [10998, 32335, 201534, 9730, 30588]\n",
      "2038  s121  07_209.xml             0     [32335, 201534, 9730, 30588, 32]\n",
      "2039  s121  07_209.xml             0         [201534, 9730, 30588, 32, 7]\n",
      "2040  s121  07_209.xml             0           [9730, 30588, 32, 7, 1446]\n",
      "2041  s121  07_209.xml             0           [30588, 32, 7, 1446, 4522]\n",
      "2042  s121  07_209.xml             0               [32, 7, 1446, 4522, 3]\n",
      "2043  s121  07_209.xml             0                [7, 1446, 4522, 3, 0]\n",
      "2044  s121  07_209.xml             0             [1446, 4522, 3, 0, 1997]\n",
      "2045  s121  07_209.xml             0               [4522, 3, 0, 1997, 10]\n",
      "2046  s121  07_209.xml             0               [3, 0, 1997, 10, 5064]\n",
      "2047  s121  07_209.xml             0            [0, 1997, 10, 5064, 9431]\n",
      "2048  s121  07_209.xml             0            [1997, 10, 5064, 9431, 3]\n",
      "2049  s121  07_209.xml             0               [10, 5064, 9431, 3, 0]\n",
      "2050  s121  07_209.xml             0            [5064, 9431, 3, 0, 26011]\n",
      "2051  s121  07_209.xml             0             [9431, 3, 0, 26011, 868]\n",
      "2052  s121  07_209.xml             0           [3, 0, 26011, 868, 201534]\n",
      "2053  s122  07_209.xml             0          [201534, 1076, 617, 650, 0]\n",
      "2054  s122  07_209.xml             0           [1076, 617, 650, 0, 60389]\n",
      "2055  s122  07_209.xml             0            [617, 650, 0, 60389, 789]\n",
      "2056  s122  07_209.xml             0              [650, 0, 60389, 789, 6]\n",
      "2057  s122  07_209.xml             0           [0, 60389, 789, 6, 201534]\n",
      "\n",
      "[2058 rows x 4 columns], [\"application for leave to appeal from 'show cause' dismissal by federal magistrate under r 44.12(1)(a) federal magistrates court rules\", 'whether failure to comply with the migration act 1958 (cth) ss 424a and 441a', 'whether breach of natural justice', 'whether improper exercise of power', 'whether failure to consider evidence favourable to applicant', 'whether bias.', 'migration'])\n",
      "('time-taken:', 2.1194050312042236)\n"
     ]
    }
   ],
   "source": [
    "sentences,catchphrases=get_statements(files[0])\n",
    "start=time.time()\n",
    "sentences, catch_words = fp.get_dataframe(pd.DataFrame(sentences),pd.DataFrame(catchphrases))\n",
    "print(sentences,catch_words)\n",
    "print(\"time-taken:\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_index_matrices(sentence_indexes):\n",
    "    length=len(sentence_indexes);\n",
    "    \n",
    "    sentence_indexes=[word_index['unk'],word_index['unk']]+sentence_indexes+[word_index['unk'],word_index['unk']];    \n",
    "    out_indices=[]\n",
    "    for i in range(2,length):\n",
    "        out_indices.append(sentence_indexes[i-2:i+3]);\n",
    "        \n",
    "    return out_indices;\n",
    "\n",
    "def prepare_label_matrices(labels):\n",
    "    length=len(labels);\n",
    "    labels=[0,0]+labels+[0,0];\n",
    "    out_labels=[];\n",
    "    \n",
    "    for i in range(2,length):\n",
    "        out_labels.append(labels[i]);\n",
    "        \n",
    "    return out_labels;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_indexes_labels(sentences,catchphrases):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    labels=[];\n",
    "    \n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "            \n",
    "    for word in sentences:\n",
    "        if word in catchphrases:\n",
    "            labels.append(1.0);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            labels.append(0.0);\n",
    "            \n",
    "    return sentence_indexes,labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_sentences(file_name):\n",
    "    temp_data=full_data[(full_data.file_id==file_name) & full_data['Id'].str.startswith(('s'))]\n",
    "    return list(temp_data.text.values)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class CatchPhraseExtractor(object):\n",
    "\n",
    "    def __init__(self, graph):\n",
    "        self.total_loss=0.0\n",
    "        self.build_graph(graph)\n",
    "\n",
    "    def build_graph(self, graph):\n",
    "        print(\"generating graph\");\n",
    "\n",
    "        with graph.as_default():            \n",
    "            self.phrases_indices = tf.placeholder(tf.int32, shape=[None,5])\n",
    "            self.phrase_labels = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            \n",
    "            #W = tf.Variable(embedding_array, dtype=tf.float32)\n",
    "            W = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            phrase_tensor = tf.nn.embedding_lookup(W, self.phrases_indices)\n",
    "            \n",
    "            #self.sentence_indices = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            #self.sentence_embeddings = tf.placeholder(tf.float32, shape=[None,200])\n",
    "            #sentence_tensor = tf.nn.embedding_lookup(self.sentence_embeddings, self.sentence_indices)\n",
    "            #sentence_tensor=tf.reshape(sentence_tensor, [tf.shape(sentence_tensor)[0], 200])\n",
    "                                                     \n",
    "            self.doc_embedding = tf.placeholder(tf.float32, shape=[1,200])\n",
    "            document_tensor = tf.tile(self.doc_embedding, [tf.shape(self.phrases_indices)[0], 1])\n",
    "            \n",
    "            phrase_tensor = tf.reshape(phrase_tensor, [tf.shape(phrase_tensor)[0], 1000])\n",
    "            conv_filter = tf.Variable(tf.truncated_normal((1000, 200), stddev=0.1));\n",
    "            word_tensor = tf.nn.relu(tf.matmul(phrase_tensor, conv_filter));\n",
    "            \n",
    "            #stacked_tensor = tf.concat(values=[word_tensor, sentence_tensor, document_tensor], axis=1)\n",
    "            stacked_tensor = tf.concat(values=[word_tensor, document_tensor], axis=1)\n",
    "            weights_input = tf.Variable(tf.random_normal((400, 100), stddev=0.2));\n",
    "            weights_input_2 = tf.Variable(tf.random_normal((100, 1), stddev=0.2));\n",
    "            \n",
    "            bias1 = tf.Variable(tf.random_normal((1, 100), stddev=0.2));\n",
    "            bias2 = tf.Variable(tf.random_normal((1, 1), stddev=0.2));\n",
    "            \n",
    "            self.sentence_values=self.MLP(stacked_tensor, weights_input, weights_input_2, bias1, bias2);\n",
    "            \n",
    "            #prediction_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.phrase_labels,logits=self.sentence_values)\n",
    "            #self.loss = tf.reduce_mean(prediction_loss)\n",
    "            self.loss=tf.reduce_mean(tf.keras.metrics.mean_squared_error(self.phrase_labels, self.sentence_values))\n",
    "            #self.loss=tf.convert_to_tensor(tf.reduce_mean(self.sentence_values));\n",
    "            \n",
    "            #using the gradient descent optimizer\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            #clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(grads)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    def MLP(self, stacked_tensor, weights_input, weights_input_2, bias1, bias2):\n",
    "        h1 = (tf.matmul(weights_input,stacked_tensor,transpose_a=True,transpose_b=True))\n",
    "        print ('#######h1 after mul: ', h1.shape)\n",
    "        #h1 = tf.add(h1, bias1)\n",
    "        #print ('#######h1_afterbias: ', h1.shape)\n",
    "        h1 = tf.nn.tanh(h1)\n",
    "        print ('#############h1.shape', h1.shape)\n",
    "        print ('###########weights_input_2.shape', weights_input_2.shape)\n",
    "        h2 = tf.matmul(weights_input_2,h1,transpose_a=True,transpose_b=False)\n",
    "        #h2 = tf.add(h2, bias2)\n",
    "        h2 = tf.nn.sigmoid(h2)\n",
    "        #h2 = tf.round(tf.transpose(h2))\n",
    "        return tf.transpose(h2)\n",
    "        #return tf.cast(tf.to_float(h2>0.5),dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def gradient_descent(self,total_loss):\n",
    "        total_loss=tf.convert_to_tensor(total_loss)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            print(variables)\n",
    "        return total_loss, tape.gradient(tf.convert_to_tensor(total_loss), variables)\n",
    "    '''     \n",
    "                                                   \n",
    "    def train(self, sess, num_steps):\n",
    "        self.init.run()\n",
    "        #print(\"Initailized\")\n",
    "        start=time.time()\n",
    "        total_loss=0.0;\n",
    "        for file in files[:20]:\n",
    "            start=time.time()\n",
    "            sentences, catchphrases=get_statements(file);\n",
    "            file_data_frame, catch_words = fp.get_dataframe(pd.DataFrame(sentences),pd.DataFrame(catchphrases))\n",
    "            #print(\"Time taken:\",time.time()-start)\n",
    "            \n",
    "            #needs sentences list\n",
    "            #sentences_list = fp.get_sentences(file) #indexes/order preserved\n",
    "            #sentence_embeddings = sen2vec(sentences_list)\n",
    "            #sentence_embeddings = get_sent_embeddings(sentences_list)\n",
    "            \n",
    "            #sentence_ids_to_indices = np.array([int(x[1:]) for x in list(file_data_frame['Id'].values)])\n",
    "            \n",
    "            #phrase inputs and labels\n",
    "            phrases_indices = np.array(list(file_data_frame['words'].values))\n",
    "            phrases_label_matrix = np.array(list(file_data_frame['is_catchword'].values))\n",
    "            \n",
    "            \n",
    "            #doc embedding\n",
    "            doc_embedding = d2v_model.docvecs[file]\n",
    "\n",
    "            \n",
    "            doc_embedding = np.reshape(doc_embedding, (1, 200))\n",
    "            #sentence_ids_to_indices = np.reshape(sentence_ids_to_indices, (sentence_ids_to_indices.shape[0], 1))\n",
    "            phrases_label_matrix = np.reshape(phrases_label_matrix, (phrases_label_matrix.shape[0], 1))\n",
    "            \n",
    "            '''\n",
    "            print ('*************Doc: ', doc_embedding.shape)\n",
    "            print ('*************phrases_indices: ', phrases_indices.shape)\n",
    "            print ('*************sentence_ids_to_indices: ', sentence_ids_to_indices.shape)\n",
    "            print ('*************phrases_label_matrix: ', phrases_label_matrix.shape)\n",
    "            print ('*************sentence_embeddings: ', sentence_embeddings.shape)           \n",
    "            \n",
    "            feed_dict = {self.doc_embedding: doc_embedding, \n",
    "                         self.phrases_indices: phrases_indices, \n",
    "                         self.phrase_labels: phrases_label_matrix, \n",
    "                         self.sentence_indices:sentence_ids_to_indices, \n",
    "                         self.sentence_embeddings: sentence_embeddings}'''\n",
    "            \n",
    "            feed_dict = {self.doc_embedding: doc_embedding, \n",
    "                         self.phrases_indices: phrases_indices, \n",
    "                         self.phrase_labels: phrases_label_matrix}\n",
    "            \n",
    "            #print(\"sending inputs\");\n",
    "            #loss_val = sess.run([self.loss], feed_dict=feed_dict)  \n",
    "            _,loss_val,outputs = sess.run([self.app,self.loss,self.sentence_values], feed_dict=feed_dict)\n",
    "\n",
    "            print('Train Finished for ', file, ' timetaken: ', time.time()-start,' loss_val=', loss_val)\n",
    "            #for i in range(len(outputs)):\n",
    "            #    print(outputs[i])\n",
    "            \n",
    "        \n",
    "    def load_embeddings(self,embedding_array):\n",
    "        sess.run(self.embedding_init, feed_dict={self.embedding_placeholder: np.asarray(list(embedding_array.values()))})\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(inputs):\n",
    "    out=np.zeros((len(inputs),200));\n",
    "    for i in range(len(inputs)):\n",
    "        try:\n",
    "            c = w2v[inputs[i]]\n",
    "        except KeyError:\n",
    "            c = np.zeros((1,200));\n",
    "        out[i]=c;\n",
    "    \n",
    "    return np.transpose(out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating graph\n",
      "('#######h1 after mul: ', TensorShape([Dimension(100), Dimension(None)]))\n",
      "('#############h1.shape', TensorShape([Dimension(100), Dimension(None)]))\n",
      "('###########weights_input_2.shape', TensorShape([Dimension(100), Dimension(1)]))\n",
      "('time taken:', 2.5033950805664062e-05)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "model = CatchPhraseExtractor(graph)\n",
    "start=time.time()\n",
    "\n",
    "#print(fp.get_dataframe(files[0]))\n",
    "#sentences_list = fp.get_file_sentences(files[0]) #indexes/order preserved\n",
    "#sentence_embeddings = sen2vec(sentences_list)\n",
    "end=time.time()\n",
    "print(\"time taken:\",end-start);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.198723077774048, ' loss_val=', 0.50694317)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.5551731586456299, ' loss_val=', 0.20376645)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 13.486531019210815, ' loss_val=', 0.21002378)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.794409990310669, ' loss_val=', 0.57115394)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 17.61236310005188, ' loss_val=', 0.18755013)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.1055359840393066, ' loss_val=', 0.2705482)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 16.059673070907593, ' loss_val=', 0.1818798)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.774461030960083, ' loss_val=', 0.14615428)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.5985100269317627, ' loss_val=', 0.5840475)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.3841660022735596, ' loss_val=', 0.36740142)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.017030954360962, ' loss_val=', 0.15670505)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.6314048767089844, ' loss_val=', 0.51346034)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 7.761023998260498, ' loss_val=', 0.541363)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.236798048019409, ' loss_val=', 0.38946018)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 8.987536191940308, ' loss_val=', 0.345914)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.9855530261993408, ' loss_val=', 0.57920784)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 5.547872066497803, ' loss_val=', 0.4017796)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 1.3710010051727295, ' loss_val=', 0.5257666)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 1.7103700637817383, ' loss_val=', 0.49974632)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 4.32307505607605, ' loss_val=', 0.23393695)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.3350720405578613, ' loss_val=', 0.19514306)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.6102211475372314, ' loss_val=', 0.27004024)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 14.572191953659058, ' loss_val=', 0.5779928)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 2.0988099575042725, ' loss_val=', 0.42372352)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 18.950263023376465, ' loss_val=', 0.064835995)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.239912986755371, ' loss_val=', 0.59400874)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 16.13365387916565, ' loss_val=', 0.27284887)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.791982889175415, ' loss_val=', 0.58533716)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.5958549976348877, ' loss_val=', 0.52228916)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.400473117828369, ' loss_val=', 0.39244932)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.0193099975585938, ' loss_val=', 0.61596096)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.6510388851165771, ' loss_val=', 0.47096965)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 7.726333141326904, ' loss_val=', 0.21440494)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.209197998046875, ' loss_val=', 0.18981989)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 5.702893018722534, ' loss_val=', 0.16080175)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.357809066772461, ' loss_val=', 0.40477994)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.6175429821014404, ' loss_val=', 0.40966508)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.7448320388793945, ' loss_val=', 0.5314198)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 0.9536099433898926, ' loss_val=', 0.4878747)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 3.058505058288574, ' loss_val=', 0.5422733)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.1313230991363525, ' loss_val=', 0.41845343)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.7109689712524414, ' loss_val=', 0.48236522)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 16.477566957473755, ' loss_val=', 0.46064767)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 2.232863187789917, ' loss_val=', 0.2243731)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 20.03177785873413, ' loss_val=', 0.23159063)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.196056842803955, ' loss_val=', 0.19800663)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 18.089027881622314, ' loss_val=', 0.6377987)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.8286550045013428, ' loss_val=', 0.32678035)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.8040850162506104, ' loss_val=', 0.34259436)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.6459221839904785, ' loss_val=', 0.25745034)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.1284980773925781, ' loss_val=', 0.52319914)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.8171799182891846, ' loss_val=', 0.3195365)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 8.511559009552002, ' loss_val=', 0.39506152)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.5331509113311768, ' loss_val=', 0.14262672)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 6.089764833450317, ' loss_val=', 0.13750353)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.3008811473846436, ' loss_val=', 0.25160378)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.881855010986328, ' loss_val=', 0.4873177)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.8186390399932861, ' loss_val=', 0.7959518)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 1.029278039932251, ' loss_val=', 0.9046242)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 3.390047073364258, ' loss_val=', 0.28283483)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.3723349571228027, ' loss_val=', 0.60508764)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.5457088947296143, ' loss_val=', 0.632527)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 13.727483987808228, ' loss_val=', 0.5470118)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.7934389114379883, ' loss_val=', 0.63592917)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 17.546157121658325, ' loss_val=', 0.58930176)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.1194570064544678, ' loss_val=', 0.5032255)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 15.75261402130127, ' loss_val=', 0.67425543)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.7567770481109619, ' loss_val=', 0.52028733)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.5662310123443604, ' loss_val=', 0.7222584)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.376971960067749, ' loss_val=', 0.86846226)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.0110321044921875, ' loss_val=', 0.7391029)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.7656950950622559, ' loss_val=', 0.73446804)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 7.659360885620117, ' loss_val=', 0.63910306)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.1691060066223145, ' loss_val=', 0.6555362)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 5.464136838912964, ' loss_val=', 0.48819548)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.174738883972168, ' loss_val=', 0.66458714)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.496194839477539, ' loss_val=', 0.6344691)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.7319960594177246, ' loss_val=', 0.6005979)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 0.9251649379730225, ' loss_val=', 0.10922368)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 2.9730350971221924, ' loss_val=', 0.57977635)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.0766749382019043, ' loss_val=', 0.21544532)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.5376667976379395, ' loss_val=', 0.4239876)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 13.585256099700928, ' loss_val=', 0.10113511)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.8062479496002197, ' loss_val=', 0.41095674)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 17.715312957763672, ' loss_val=', 0.31818762)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.1083879470825195, ' loss_val=', 0.19226667)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 15.983786821365356, ' loss_val=', 0.06748198)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.7511749267578125, ' loss_val=', 0.46147192)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.6322031021118164, ' loss_val=', 0.10465203)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.401757001876831, ' loss_val=', 0.09160807)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 0.9998109340667725, ' loss_val=', 0.506294)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.6326320171356201, ' loss_val=', 0.3337362)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 7.644528865814209, ' loss_val=', 0.17799397)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.2862370014190674, ' loss_val=', 0.24319592)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 5.45558500289917, ' loss_val=', 0.19074659)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.1671929359436035, ' loss_val=', 0.16831753)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.503730058670044, ' loss_val=', 0.28004447)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.7338218688964844, ' loss_val=', 0.58018005)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 0.9212942123413086, ' loss_val=', 0.40218475)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 2.942647933959961, ' loss_val=', 0.090933844)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.0667240619659424, ' loss_val=', 0.08203802)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.5423691272735596, ' loss_val=', 0.021650719)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 14.041254997253418, ' loss_val=', 0.080914535)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.8505401611328125, ' loss_val=', 0.05687873)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 17.63480806350708, ' loss_val=', 0.040564556)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.1190578937530518, ' loss_val=', 0.038936235)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 15.834655046463013, ' loss_val=', 0.065342896)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.7518630027770996, ' loss_val=', 0.25818354)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.5722999572753906, ' loss_val=', 0.091836244)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.3848390579223633, ' loss_val=', 0.04705353)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.005741834640503, ' loss_val=', 0.0646709)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.6893079280853271, ' loss_val=', 0.03801004)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 8.143165111541748, ' loss_val=', 0.11316507)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.6591238975524902, ' loss_val=', 0.039851274)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 6.865461111068726, ' loss_val=', 0.06490504)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.3070580959320068, ' loss_val=', 0.06129291)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.9239959716796875, ' loss_val=', 0.12830698)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.7466249465942383, ' loss_val=', 0.050962888)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 1.2244551181793213, ' loss_val=', 0.66694915)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 3.5995030403137207, ' loss_val=', 0.09581954)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.287583112716675, ' loss_val=', 0.5536494)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.595163106918335, ' loss_val=', 0.47570688)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 15.016751050949097, ' loss_val=', 0.13626722)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.9596920013427734, ' loss_val=', 0.16520518)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 19.967313051223755, ' loss_val=', 0.15454973)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.318615198135376, ' loss_val=', 0.13825177)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 18.022451877593994, ' loss_val=', 0.1676374)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.9114201068878174, ' loss_val=', 0.118342206)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 3.4100630283355713, ' loss_val=', 0.32210866)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.7701539993286133, ' loss_val=', 0.31547168)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.1258211135864258, ' loss_val=', 0.39316833)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.8125228881835938, ' loss_val=', 0.22016867)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 8.507917165756226, ' loss_val=', 0.1559953)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.6269280910491943, ' loss_val=', 0.26364416)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 5.499627113342285, ' loss_val=', 0.21801639)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.1707360744476318, ' loss_val=', 0.2399616)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.480316162109375, ' loss_val=', 0.25162697)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.7273390293121338, ' loss_val=', 0.24201076)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 0.9253289699554443, ' loss_val=', 0.10820696)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 3.125465154647827, ' loss_val=', 0.13915884)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.077587127685547, ' loss_val=', 0.24348468)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.5374720096588135, ' loss_val=', 0.24650122)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 15.37272596359253, ' loss_val=', 0.23098128)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.872403860092163, ' loss_val=', 0.100174904)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 18.060153007507324, ' loss_val=', 0.30123395)\n",
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.126883029937744, ' loss_val=', 0.16966124)\n",
      "('Train Finished for ', '07_1402.xml', ' timetaken: ', 15.9510178565979, ' loss_val=', 0.21928857)\n",
      "('Train Finished for ', '06_713.xml', ' timetaken: ', 0.7543041706085205, ' loss_val=', 0.20275293)\n",
      "('Train Finished for ', '06_1260.xml', ' timetaken: ', 2.5694239139556885, ' loss_val=', 0.26620013)\n",
      "('Train Finished for ', '08_711.xml', ' timetaken: ', 2.400926113128662, ' loss_val=', 0.37320396)\n",
      "('Train Finished for ', '08_1030.xml', ' timetaken: ', 1.004127025604248, ' loss_val=', 0.2849319)\n",
      "('Train Finished for ', '06_707.xml', ' timetaken: ', 1.6268229484558105, ' loss_val=', 0.25554824)\n",
      "('Train Finished for ', '06_1326.xml', ' timetaken: ', 7.709443092346191, ' loss_val=', 0.2464821)\n",
      "('Train Finished for ', '08_712.xml', ' timetaken: ', 3.141896963119507, ' loss_val=', 0.24092984)\n",
      "('Train Finished for ', '06_613.xml', ' timetaken: ', 5.572718143463135, ' loss_val=', 0.5176642)\n",
      "('Train Finished for ', '06_1641.xml', ' timetaken: ', 1.219423770904541, ' loss_val=', 0.66651344)\n",
      "('Train Finished for ', '09_1156.xml', ' timetaken: ', 3.8040359020233154, ' loss_val=', 0.5574649)\n",
      "('Train Finished for ', '07_959.xml', ' timetaken: ', 0.7941851615905762, ' loss_val=', 0.20863478)\n",
      "('Train Finished for ', '06_35.xml', ' timetaken: ', 0.9282219409942627, ' loss_val=', 0.049138382)\n",
      "('Train Finished for ', '07_1486.xml', ' timetaken: ', 3.1842429637908936, ' loss_val=', 0.49687156)\n",
      "('Train Finished for ', '07_209.xml', ' timetaken: ', 2.2729179859161377, ' loss_val=', 0.11921199)\n",
      "('Train Finished for ', '06_159.xml', ' timetaken: ', 0.546273946762085, ' loss_val=', 0.6224284)\n",
      "('Train Finished for ', '09_245.xml', ' timetaken: ', 14.88170599937439, ' loss_val=', 0.289151)\n",
      "('Train Finished for ', '09_636.xml', ' timetaken: ', 1.8772079944610596, ' loss_val=', 0.56180257)\n",
      "('Train Finished for ', '06_460.xml', ' timetaken: ', 19.47280216217041, ' loss_val=', 0.39728957)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train Finished for ', '06_1396.xml', ' timetaken: ', 2.294508934020996, ' loss_val=', 0.4519029)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "  \n",
    "    #model.load_embeddings(embedding_array)    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    #global_step = tf.train.get_or_create_global_step()\n",
    "    for i in range(20):\n",
    "        model.train(sess, 20)\n",
    "    #optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
