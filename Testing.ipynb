{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=2;\n",
    "stop_words_list=set(stopwords.words('english'))\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        word_vocab = set()  # not using list to avoid duplicate entry\n",
    "        word_count = {};\n",
    "        word_index={};\n",
    "\n",
    "        word2vector = {}\n",
    "        count=0;\n",
    "        for line in f:\n",
    "            line_ = line.strip()  # Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word_index[words_Vec[0]]=count;\n",
    "            count=count+1;\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:], dtype=float)\n",
    "    print(\"Total Words in DataSet:\", len(word_vocab))\n",
    "    return word_vocab, word2vector, word_index\n",
    "\n",
    "vocab, w2v,word_index = read_data(\"glove_6B_200d.txt\");\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=pd.read_csv('./data.csv');\n",
    "print(np.shape(full_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_data.head(5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=full_data[:1000]\n",
    "\n",
    "\n",
    "# for index, row in data.iterrows():\n",
    "#     text_tokens=word_tokenize(row['text']);\n",
    "#     for token in text_tokens:\n",
    "#         if token in word_count:\n",
    "#             print(\"Already exists\",token)\n",
    "#             word_count[token]=word_count.get(token)+1;\n",
    "#         else:\n",
    "#             word_count[token]=1;\n",
    "            \n",
    "# print(word_count['the'])\n",
    "# print(data.head(5));\n",
    "# data.to_csv('sample_data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_df=full_data[full_data['Id'].str.startswith(('c'))]\n",
    "sent_df=full_data[full_data['Id'].str.startswith(('s'))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in data.iterrows()[:10]:\n",
    "#     print ( row['file_id'],row['Id'],row['text'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#utiil\n",
    "def get_word_count(string):\n",
    "    tokens = string.split()\n",
    "    n_tokens = len(tokens)\n",
    "    return n_tokens  \n",
    "\n",
    "def ReLU(x):\n",
    "    return abs(x) * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(inputs):\n",
    "    out=np.zeros((len(inputs),200));\n",
    "    for i in range(len(inputs)):\n",
    "        try:\n",
    "            c = w2v[inputs[i]]\n",
    "        except KeyError:\n",
    "            c = np.zeros((1,200));\n",
    "        out[i]=c;\n",
    "    \n",
    "    return np.transpose(out);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_indexes(sentences):\n",
    "    \n",
    "    sentence_indexes=[];\n",
    "    for word in sentences:\n",
    "        if word in word_index:\n",
    "            sentence_indexes.append(word_index[word]);\n",
    "        else:\n",
    "            #print(\"unknown word\",word)\n",
    "            sentence_indexes.append(word_index['unk']);\n",
    "        \n",
    "    return sentence_indexes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_row(file_id,Id,text,catch_words):\n",
    "    text=text.lower();\n",
    "    \n",
    "    word_count=get_word_count(text);\n",
    "    words=text.split()\n",
    "    input_filter=np.random.rand(300,200*(2*window_size+1));\n",
    "    sentence_vector=np.zeros((300,1));\n",
    "    count=0;\n",
    "    rows_list=[]\n",
    "    if word_count>2*window_size+1:\n",
    "        for i in range(window_size,word_count-window_size):\n",
    "            \n",
    "            inputs=words[i-window_size:i+window_size+1];\n",
    "            inputs=lookup_indexes(inputs)\n",
    "            is_catchword = 1 if words[i] in catch_words else 0\n",
    "            #print(inputs);\n",
    "            #input_features=ReLU(get_features(inputs));\n",
    "            #input_features=np.reshape(input_features,(200*(2*window_size+1),1));\n",
    "            #input_features=ReLU(np.matmul(input_filter,input_features));\n",
    "            #print(np.shape(input_features));\n",
    "            #print(\"Entered:\",{\"file_id\",file_id, \"Id\",Id, \"words\",inputs, \"is_catchword\",is_catchword})\n",
    "            rows_list.append({\"file_id\":file_id, \"Id\":Id, \"words\":inputs, \"is_catchword\":is_catchword})\n",
    "            #print(file_id,Id,inputs);\n",
    "            #sentence_vector=np.add(sentence_vector,input_features);\n",
    "            #count=count+1;\n",
    "        #sentence_vector= np.true_divide(sentence_vector, 4);\n",
    "        #print(np.shape(sentence_vector));\n",
    "    \n",
    "    return rows_list\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catch_words(catch_df):\n",
    "    catch_words=[]\n",
    "    for index, row in catch_df.iterrows():\n",
    "        \n",
    "        words=word_tokenize(row['text']);\n",
    "        temp=[word for word in words if not word in stop_words_list]\n",
    "        catch_words=catch_words+temp;\n",
    "    return catch_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(file_name):\n",
    "    file_data=full_data[full_data.file_id==file_name];\n",
    "    catch_df=file_data[file_data['Id'].str.startswith(('c'))]\n",
    "    catch_words=get_catch_words(catch_df);\n",
    "    \n",
    "    sent_df=file_data[file_data['Id'].str.startswith(('s'))]\n",
    "    sent_df = sent_df[:-1]\n",
    "    file_dataframe=[];\n",
    "    count=0;\n",
    "    for index, row in sent_df.iterrows():\n",
    "        count=count+1;\n",
    "        #print(get_features_row_dummy(row['file_id'],row['Id'],row['text']))\n",
    "        temp=get_features_row(row['file_id'],row['Id'],row['text'],catch_words)\n",
    "        if len(temp)>0:\n",
    "            file_dataframe=file_dataframe+temp;\n",
    "\n",
    "    return pd.DataFrame(file_dataframe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_dataframe('08_1056.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
